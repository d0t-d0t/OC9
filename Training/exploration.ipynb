{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3e28e24",
   "metadata": {},
   "source": [
    "## Import initiaux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34c89d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "import pickle\n",
    "import time\n",
    "import mlflow\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "ARTICLES_PATH = \"Datas/articles_metadata.csv\"  \n",
    "CLICKS_SAMPLE_PATH = \"Datas/clicks_sample.csv\"     \n",
    "CLICKS__PATH = \"Datas/clicks/\"         \n",
    "EMBEDING_PATH = \"Datas/articles_embeddings.pickle\" \n",
    "RATING_PREPROC_DF_PATH='Datas/rating_preprocess_df.pkl'\n",
    "CANDIDATE_PREPROC_DF_PATH='Datas/candidate_preprocess_df.pkl'\n",
    "\n",
    "debug=False\n",
    "\n",
    "    \n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "pd.set_option('display.width' ,2000)\n",
    "pd.set_option('display.precision', 1)\n",
    "pd.set_option('display.max_colwidth', 30)  \n",
    "pd.set_option('display.expand_frame_repr', True) \n",
    "pd.set_option('display.float_format', '{:,.2f}'.format)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7857e6",
   "metadata": {},
   "source": [
    "# I. Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc021aa",
   "metadata": {},
   "source": [
    "\n",
    "## A. Articles_metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d502e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Articles shape: (364047, 5)\n",
      "\n",
      "ARTICLES METADATA:\n",
      "Number of articles: 364047\n",
      "Number of columns: 5\n",
      "\n",
      "Columns:\n",
      "  - article_id\n",
      "  - category_id\n",
      "  - created_at_ts\n",
      "  - publisher_id\n",
      "  - words_count\n",
      "\n",
      "articles data head:\n",
      "   article_id  category_id  created_at_ts  publisher_id  words_count\n",
      "0           0            0  1513144419000             0          168\n",
      "1           1            1  1405341936000             0          189\n",
      "2           2            1  1408667706000             0          250\n",
      "3           3            1  1408468313000             0          230\n",
      "4           4            1  1407071171000             0          162\n",
      "Articles dataset:\n",
      "article_id       int64\n",
      "category_id      int64\n",
      "created_at_ts    int64\n",
      "publisher_id     int64\n",
      "words_count      int64\n",
      "dtype: object\n",
      "Missing Values:\n",
      "article_id       0\n",
      "category_id      0\n",
      "created_at_ts    0\n",
      "publisher_id     0\n",
      "words_count      0\n",
      "dtype: int64\n",
      "Duplicate articles: 0\n"
     ]
    }
   ],
   "source": [
    "# Load articles metadata\n",
    "articles_df = pd.read_csv(ARTICLES_PATH)\n",
    "print(f\"Articles shape: {articles_df.shape}\")\n",
    "\n",
    "# Articles dataset info\n",
    "print(\"\\nARTICLES METADATA:\")\n",
    "print(f\"Number of articles: {len(articles_df)}\")\n",
    "print(f\"Number of columns: {len(articles_df.columns)}\")\n",
    "print(\"\\nColumns:\")\n",
    "for col in articles_df.columns:\n",
    "    print(f\"  - {col}\")\n",
    "\n",
    "print(\"\\narticles data head:\")\n",
    "print(articles_df.head())\n",
    "\n",
    "print(\"Articles dataset:\")\n",
    "print(articles_df.dtypes)\n",
    "\n",
    "print(\"Missing Values:\")\n",
    "print(articles_df.isnull().sum())\n",
    "\n",
    "print(f\"Duplicate articles: {articles_df.duplicated().sum()}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ef13d3",
   "metadata": {},
   "source": [
    "la pluspart des informations vont nous êtres inutiles:\n",
    "    - l'id servira au merge avec les autres df \n",
    "    - on peut conserver  la catégorie même si elle sera probablement redondante avec l'embediing\n",
    "    - words_count pourrait servir à pondérer le temps de rétentention, si on peut le calculer avec la longueur de l'article"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285c72af",
   "metadata": {},
   "source": [
    "## B. Clicks_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83983dd",
   "metadata": {},
   "source": [
    "### Load datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4267e469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: Datas/clicks/clicks_hour_000.csv\n",
      "Loading: Datas/clicks/clicks_hour_001.csv\n",
      "Loading: Datas/clicks/clicks_hour_002.csv\n",
      "Loading: Datas/clicks/clicks_hour_003.csv\n",
      "Loading: Datas/clicks/clicks_hour_004.csv\n",
      "Loading: Datas/clicks/clicks_hour_005.csv\n",
      "Loading: Datas/clicks/clicks_hour_006.csv\n",
      "Loading: Datas/clicks/clicks_hour_007.csv\n",
      "Loading: Datas/clicks/clicks_hour_008.csv\n",
      "Loading: Datas/clicks/clicks_hour_009.csv\n",
      "Loading: Datas/clicks/clicks_hour_010.csv\n",
      "Loading: Datas/clicks/clicks_hour_011.csv\n",
      "Loading: Datas/clicks/clicks_hour_012.csv\n",
      "Loading: Datas/clicks/clicks_hour_013.csv\n",
      "Loading: Datas/clicks/clicks_hour_014.csv\n",
      "Loading: Datas/clicks/clicks_hour_015.csv\n",
      "Loading: Datas/clicks/clicks_hour_016.csv\n",
      "Loading: Datas/clicks/clicks_hour_017.csv\n",
      "Loading: Datas/clicks/clicks_hour_018.csv\n",
      "Loading: Datas/clicks/clicks_hour_019.csv\n",
      "Loading: Datas/clicks/clicks_hour_020.csv\n",
      "Loading: Datas/clicks/clicks_hour_021.csv\n",
      "Loading: Datas/clicks/clicks_hour_022.csv\n",
      "Loading: Datas/clicks/clicks_hour_023.csv\n",
      "Loading: Datas/clicks/clicks_hour_024.csv\n",
      "Loading: Datas/clicks/clicks_hour_025.csv\n",
      "Loading: Datas/clicks/clicks_hour_026.csv\n",
      "Loading: Datas/clicks/clicks_hour_027.csv\n",
      "Loading: Datas/clicks/clicks_hour_028.csv\n",
      "Loading: Datas/clicks/clicks_hour_029.csv\n",
      "Loading: Datas/clicks/clicks_hour_030.csv\n",
      "Loading: Datas/clicks/clicks_hour_031.csv\n",
      "Loading: Datas/clicks/clicks_hour_032.csv\n",
      "Loading: Datas/clicks/clicks_hour_033.csv\n",
      "Loading: Datas/clicks/clicks_hour_034.csv\n",
      "Loading: Datas/clicks/clicks_hour_035.csv\n",
      "Loading: Datas/clicks/clicks_hour_036.csv\n",
      "Loading: Datas/clicks/clicks_hour_037.csv\n",
      "Loading: Datas/clicks/clicks_hour_038.csv\n",
      "Loading: Datas/clicks/clicks_hour_039.csv\n",
      "Loading: Datas/clicks/clicks_hour_040.csv\n",
      "Loading: Datas/clicks/clicks_hour_041.csv\n",
      "Loading: Datas/clicks/clicks_hour_042.csv\n",
      "Loading: Datas/clicks/clicks_hour_043.csv\n",
      "Loading: Datas/clicks/clicks_hour_044.csv\n",
      "Loading: Datas/clicks/clicks_hour_045.csv\n",
      "Loading: Datas/clicks/clicks_hour_046.csv\n",
      "Loading: Datas/clicks/clicks_hour_047.csv\n",
      "Loading: Datas/clicks/clicks_hour_048.csv\n",
      "Loading: Datas/clicks/clicks_hour_049.csv\n",
      "Loading: Datas/clicks/clicks_hour_050.csv\n",
      "Loading: Datas/clicks/clicks_hour_051.csv\n",
      "Loading: Datas/clicks/clicks_hour_052.csv\n",
      "Loading: Datas/clicks/clicks_hour_053.csv\n",
      "Loading: Datas/clicks/clicks_hour_054.csv\n",
      "Loading: Datas/clicks/clicks_hour_055.csv\n",
      "Loading: Datas/clicks/clicks_hour_056.csv\n",
      "Loading: Datas/clicks/clicks_hour_057.csv\n",
      "Loading: Datas/clicks/clicks_hour_058.csv\n",
      "Loading: Datas/clicks/clicks_hour_059.csv\n",
      "Loading: Datas/clicks/clicks_hour_060.csv\n",
      "Loading: Datas/clicks/clicks_hour_061.csv\n",
      "Loading: Datas/clicks/clicks_hour_062.csv\n",
      "Loading: Datas/clicks/clicks_hour_063.csv\n",
      "Loading: Datas/clicks/clicks_hour_064.csv\n",
      "Loading: Datas/clicks/clicks_hour_065.csv\n",
      "Loading: Datas/clicks/clicks_hour_066.csv\n",
      "Loading: Datas/clicks/clicks_hour_067.csv\n",
      "Loading: Datas/clicks/clicks_hour_068.csv\n",
      "Loading: Datas/clicks/clicks_hour_069.csv\n",
      "Loading: Datas/clicks/clicks_hour_070.csv\n",
      "Loading: Datas/clicks/clicks_hour_071.csv\n",
      "Loading: Datas/clicks/clicks_hour_072.csv\n",
      "Loading: Datas/clicks/clicks_hour_073.csv\n",
      "Loading: Datas/clicks/clicks_hour_074.csv\n",
      "Loading: Datas/clicks/clicks_hour_075.csv\n",
      "Loading: Datas/clicks/clicks_hour_076.csv\n",
      "Loading: Datas/clicks/clicks_hour_077.csv\n",
      "Loading: Datas/clicks/clicks_hour_078.csv\n",
      "Loading: Datas/clicks/clicks_hour_079.csv\n",
      "Loading: Datas/clicks/clicks_hour_080.csv\n",
      "Loading: Datas/clicks/clicks_hour_081.csv\n",
      "Loading: Datas/clicks/clicks_hour_082.csv\n",
      "Loading: Datas/clicks/clicks_hour_083.csv\n",
      "Loading: Datas/clicks/clicks_hour_084.csv\n",
      "Loading: Datas/clicks/clicks_hour_085.csv\n",
      "Loading: Datas/clicks/clicks_hour_086.csv\n",
      "Loading: Datas/clicks/clicks_hour_087.csv\n",
      "Loading: Datas/clicks/clicks_hour_088.csv\n",
      "Loading: Datas/clicks/clicks_hour_089.csv\n",
      "Loading: Datas/clicks/clicks_hour_090.csv\n",
      "Loading: Datas/clicks/clicks_hour_091.csv\n",
      "Loading: Datas/clicks/clicks_hour_092.csv\n",
      "Loading: Datas/clicks/clicks_hour_093.csv\n",
      "Loading: Datas/clicks/clicks_hour_094.csv\n",
      "Loading: Datas/clicks/clicks_hour_095.csv\n",
      "Loading: Datas/clicks/clicks_hour_096.csv\n",
      "Loading: Datas/clicks/clicks_hour_097.csv\n",
      "Loading: Datas/clicks/clicks_hour_098.csv\n",
      "Loading: Datas/clicks/clicks_hour_099.csv\n",
      "Loading: Datas/clicks/clicks_hour_100.csv\n",
      "Loading: Datas/clicks/clicks_hour_101.csv\n",
      "Loading: Datas/clicks/clicks_hour_102.csv\n",
      "Loading: Datas/clicks/clicks_hour_103.csv\n",
      "Loading: Datas/clicks/clicks_hour_104.csv\n",
      "Loading: Datas/clicks/clicks_hour_105.csv\n",
      "Loading: Datas/clicks/clicks_hour_106.csv\n",
      "Loading: Datas/clicks/clicks_hour_107.csv\n",
      "Loading: Datas/clicks/clicks_hour_108.csv\n",
      "Loading: Datas/clicks/clicks_hour_109.csv\n",
      "Loading: Datas/clicks/clicks_hour_110.csv\n",
      "Loading: Datas/clicks/clicks_hour_111.csv\n",
      "Loading: Datas/clicks/clicks_hour_112.csv\n",
      "Loading: Datas/clicks/clicks_hour_113.csv\n",
      "Loading: Datas/clicks/clicks_hour_114.csv\n",
      "Loading: Datas/clicks/clicks_hour_115.csv\n",
      "Loading: Datas/clicks/clicks_hour_116.csv\n",
      "Loading: Datas/clicks/clicks_hour_117.csv\n",
      "Loading: Datas/clicks/clicks_hour_118.csv\n",
      "Loading: Datas/clicks/clicks_hour_119.csv\n",
      "Loading: Datas/clicks/clicks_hour_120.csv\n",
      "Loading: Datas/clicks/clicks_hour_121.csv\n",
      "Loading: Datas/clicks/clicks_hour_122.csv\n",
      "Loading: Datas/clicks/clicks_hour_123.csv\n",
      "Loading: Datas/clicks/clicks_hour_124.csv\n",
      "Loading: Datas/clicks/clicks_hour_125.csv\n",
      "Loading: Datas/clicks/clicks_hour_126.csv\n",
      "Loading: Datas/clicks/clicks_hour_127.csv\n",
      "Loading: Datas/clicks/clicks_hour_128.csv\n",
      "Loading: Datas/clicks/clicks_hour_129.csv\n",
      "Loading: Datas/clicks/clicks_hour_130.csv\n",
      "Loading: Datas/clicks/clicks_hour_131.csv\n",
      "Loading: Datas/clicks/clicks_hour_132.csv\n",
      "Loading: Datas/clicks/clicks_hour_133.csv\n",
      "Loading: Datas/clicks/clicks_hour_134.csv\n",
      "Loading: Datas/clicks/clicks_hour_135.csv\n",
      "Loading: Datas/clicks/clicks_hour_136.csv\n",
      "Loading: Datas/clicks/clicks_hour_137.csv\n",
      "Loading: Datas/clicks/clicks_hour_138.csv\n",
      "Loading: Datas/clicks/clicks_hour_139.csv\n",
      "Loading: Datas/clicks/clicks_hour_140.csv\n",
      "Loading: Datas/clicks/clicks_hour_141.csv\n",
      "Loading: Datas/clicks/clicks_hour_142.csv\n",
      "Loading: Datas/clicks/clicks_hour_143.csv\n",
      "Loading: Datas/clicks/clicks_hour_144.csv\n",
      "Loading: Datas/clicks/clicks_hour_145.csv\n",
      "Loading: Datas/clicks/clicks_hour_146.csv\n",
      "Loading: Datas/clicks/clicks_hour_147.csv\n",
      "Loading: Datas/clicks/clicks_hour_148.csv\n",
      "Loading: Datas/clicks/clicks_hour_149.csv\n",
      "Loading: Datas/clicks/clicks_hour_150.csv\n",
      "Loading: Datas/clicks/clicks_hour_151.csv\n",
      "Loading: Datas/clicks/clicks_hour_152.csv\n",
      "Loading: Datas/clicks/clicks_hour_153.csv\n",
      "Loading: Datas/clicks/clicks_hour_154.csv\n",
      "Loading: Datas/clicks/clicks_hour_155.csv\n",
      "Loading: Datas/clicks/clicks_hour_156.csv\n",
      "Loading: Datas/clicks/clicks_hour_157.csv\n",
      "Loading: Datas/clicks/clicks_hour_158.csv\n",
      "Loading: Datas/clicks/clicks_hour_159.csv\n",
      "Loading: Datas/clicks/clicks_hour_160.csv\n",
      "Loading: Datas/clicks/clicks_hour_161.csv\n",
      "Loading: Datas/clicks/clicks_hour_162.csv\n",
      "Loading: Datas/clicks/clicks_hour_163.csv\n",
      "Loading: Datas/clicks/clicks_hour_164.csv\n",
      "Loading: Datas/clicks/clicks_hour_165.csv\n",
      "Loading: Datas/clicks/clicks_hour_166.csv\n",
      "Loading: Datas/clicks/clicks_hour_167.csv\n",
      "Loading: Datas/clicks/clicks_hour_168.csv\n",
      "Loading: Datas/clicks/clicks_hour_169.csv\n",
      "Loading: Datas/clicks/clicks_hour_170.csv\n",
      "Loading: Datas/clicks/clicks_hour_171.csv\n",
      "Loading: Datas/clicks/clicks_hour_172.csv\n",
      "Loading: Datas/clicks/clicks_hour_173.csv\n",
      "Loading: Datas/clicks/clicks_hour_174.csv\n",
      "Loading: Datas/clicks/clicks_hour_175.csv\n",
      "Loading: Datas/clicks/clicks_hour_176.csv\n",
      "Loading: Datas/clicks/clicks_hour_177.csv\n",
      "Loading: Datas/clicks/clicks_hour_178.csv\n",
      "Loading: Datas/clicks/clicks_hour_179.csv\n",
      "Loading: Datas/clicks/clicks_hour_180.csv\n",
      "Loading: Datas/clicks/clicks_hour_181.csv\n",
      "Loading: Datas/clicks/clicks_hour_182.csv\n",
      "Loading: Datas/clicks/clicks_hour_183.csv\n",
      "Loading: Datas/clicks/clicks_hour_184.csv\n",
      "Loading: Datas/clicks/clicks_hour_185.csv\n",
      "Loading: Datas/clicks/clicks_hour_186.csv\n",
      "Loading: Datas/clicks/clicks_hour_187.csv\n",
      "Loading: Datas/clicks/clicks_hour_188.csv\n",
      "Loading: Datas/clicks/clicks_hour_189.csv\n",
      "Loading: Datas/clicks/clicks_hour_190.csv\n",
      "Loading: Datas/clicks/clicks_hour_191.csv\n",
      "Loading: Datas/clicks/clicks_hour_192.csv\n",
      "Loading: Datas/clicks/clicks_hour_193.csv\n",
      "Loading: Datas/clicks/clicks_hour_194.csv\n",
      "Loading: Datas/clicks/clicks_hour_195.csv\n",
      "Loading: Datas/clicks/clicks_hour_196.csv\n",
      "Loading: Datas/clicks/clicks_hour_197.csv\n",
      "Loading: Datas/clicks/clicks_hour_198.csv\n",
      "Loading: Datas/clicks/clicks_hour_199.csv\n",
      "Loading: Datas/clicks/clicks_hour_200.csv\n",
      "Loading: Datas/clicks/clicks_hour_201.csv\n",
      "Loading: Datas/clicks/clicks_hour_202.csv\n",
      "Loading: Datas/clicks/clicks_hour_203.csv\n",
      "Loading: Datas/clicks/clicks_hour_204.csv\n",
      "Loading: Datas/clicks/clicks_hour_205.csv\n",
      "Loading: Datas/clicks/clicks_hour_206.csv\n",
      "Loading: Datas/clicks/clicks_hour_207.csv\n",
      "Loading: Datas/clicks/clicks_hour_208.csv\n",
      "Loading: Datas/clicks/clicks_hour_209.csv\n",
      "Loading: Datas/clicks/clicks_hour_210.csv\n",
      "Loading: Datas/clicks/clicks_hour_211.csv\n",
      "Loading: Datas/clicks/clicks_hour_212.csv\n",
      "Loading: Datas/clicks/clicks_hour_213.csv\n",
      "Loading: Datas/clicks/clicks_hour_214.csv\n",
      "Loading: Datas/clicks/clicks_hour_215.csv\n",
      "Loading: Datas/clicks/clicks_hour_216.csv\n",
      "Loading: Datas/clicks/clicks_hour_217.csv\n",
      "Loading: Datas/clicks/clicks_hour_218.csv\n",
      "Loading: Datas/clicks/clicks_hour_219.csv\n",
      "Loading: Datas/clicks/clicks_hour_220.csv\n",
      "Loading: Datas/clicks/clicks_hour_221.csv\n",
      "Loading: Datas/clicks/clicks_hour_222.csv\n",
      "Loading: Datas/clicks/clicks_hour_223.csv\n",
      "Loading: Datas/clicks/clicks_hour_224.csv\n",
      "Loading: Datas/clicks/clicks_hour_225.csv\n",
      "Loading: Datas/clicks/clicks_hour_226.csv\n",
      "Loading: Datas/clicks/clicks_hour_227.csv\n",
      "Loading: Datas/clicks/clicks_hour_228.csv\n",
      "Loading: Datas/clicks/clicks_hour_229.csv\n",
      "Loading: Datas/clicks/clicks_hour_230.csv\n",
      "Loading: Datas/clicks/clicks_hour_231.csv\n",
      "Loading: Datas/clicks/clicks_hour_232.csv\n",
      "Loading: Datas/clicks/clicks_hour_233.csv\n",
      "Loading: Datas/clicks/clicks_hour_234.csv\n",
      "Loading: Datas/clicks/clicks_hour_235.csv\n",
      "Loading: Datas/clicks/clicks_hour_236.csv\n",
      "Loading: Datas/clicks/clicks_hour_237.csv\n",
      "Loading: Datas/clicks/clicks_hour_238.csv\n",
      "Loading: Datas/clicks/clicks_hour_239.csv\n",
      "Loading: Datas/clicks/clicks_hour_240.csv\n",
      "Loading: Datas/clicks/clicks_hour_241.csv\n",
      "Loading: Datas/clicks/clicks_hour_242.csv\n",
      "Loading: Datas/clicks/clicks_hour_243.csv\n",
      "Loading: Datas/clicks/clicks_hour_244.csv\n",
      "Loading: Datas/clicks/clicks_hour_245.csv\n",
      "Loading: Datas/clicks/clicks_hour_246.csv\n",
      "Loading: Datas/clicks/clicks_hour_247.csv\n",
      "Loading: Datas/clicks/clicks_hour_248.csv\n",
      "Loading: Datas/clicks/clicks_hour_249.csv\n",
      "Loading: Datas/clicks/clicks_hour_250.csv\n",
      "Loading: Datas/clicks/clicks_hour_251.csv\n",
      "Loading: Datas/clicks/clicks_hour_252.csv\n",
      "Loading: Datas/clicks/clicks_hour_253.csv\n",
      "Loading: Datas/clicks/clicks_hour_254.csv\n",
      "Loading: Datas/clicks/clicks_hour_255.csv\n",
      "Loading: Datas/clicks/clicks_hour_256.csv\n",
      "Loading: Datas/clicks/clicks_hour_257.csv\n",
      "Loading: Datas/clicks/clicks_hour_258.csv\n",
      "Loading: Datas/clicks/clicks_hour_259.csv\n",
      "Loading: Datas/clicks/clicks_hour_260.csv\n",
      "Loading: Datas/clicks/clicks_hour_261.csv\n",
      "Loading: Datas/clicks/clicks_hour_262.csv\n",
      "Loading: Datas/clicks/clicks_hour_263.csv\n",
      "Loading: Datas/clicks/clicks_hour_264.csv\n",
      "Loading: Datas/clicks/clicks_hour_265.csv\n",
      "Loading: Datas/clicks/clicks_hour_266.csv\n",
      "Loading: Datas/clicks/clicks_hour_267.csv\n",
      "Loading: Datas/clicks/clicks_hour_268.csv\n",
      "Loading: Datas/clicks/clicks_hour_269.csv\n",
      "Loading: Datas/clicks/clicks_hour_270.csv\n",
      "Loading: Datas/clicks/clicks_hour_271.csv\n",
      "Loading: Datas/clicks/clicks_hour_272.csv\n",
      "Loading: Datas/clicks/clicks_hour_273.csv\n",
      "Loading: Datas/clicks/clicks_hour_274.csv\n",
      "Loading: Datas/clicks/clicks_hour_275.csv\n",
      "Loading: Datas/clicks/clicks_hour_276.csv\n",
      "Loading: Datas/clicks/clicks_hour_277.csv\n",
      "Loading: Datas/clicks/clicks_hour_278.csv\n",
      "Loading: Datas/clicks/clicks_hour_279.csv\n",
      "Loading: Datas/clicks/clicks_hour_280.csv\n",
      "Loading: Datas/clicks/clicks_hour_281.csv\n",
      "Loading: Datas/clicks/clicks_hour_282.csv\n",
      "Loading: Datas/clicks/clicks_hour_283.csv\n",
      "Loading: Datas/clicks/clicks_hour_284.csv\n",
      "Loading: Datas/clicks/clicks_hour_285.csv\n",
      "Loading: Datas/clicks/clicks_hour_286.csv\n",
      "Loading: Datas/clicks/clicks_hour_287.csv\n",
      "Loading: Datas/clicks/clicks_hour_288.csv\n",
      "Loading: Datas/clicks/clicks_hour_289.csv\n",
      "Loading: Datas/clicks/clicks_hour_290.csv\n",
      "Loading: Datas/clicks/clicks_hour_291.csv\n",
      "Loading: Datas/clicks/clicks_hour_292.csv\n",
      "Loading: Datas/clicks/clicks_hour_293.csv\n",
      "Loading: Datas/clicks/clicks_hour_294.csv\n",
      "Loading: Datas/clicks/clicks_hour_295.csv\n",
      "Loading: Datas/clicks/clicks_hour_296.csv\n",
      "Loading: Datas/clicks/clicks_hour_297.csv\n",
      "Loading: Datas/clicks/clicks_hour_298.csv\n",
      "Loading: Datas/clicks/clicks_hour_299.csv\n",
      "Loading: Datas/clicks/clicks_hour_300.csv\n",
      "Loading: Datas/clicks/clicks_hour_301.csv\n",
      "Loading: Datas/clicks/clicks_hour_302.csv\n",
      "Loading: Datas/clicks/clicks_hour_303.csv\n",
      "Loading: Datas/clicks/clicks_hour_304.csv\n",
      "Loading: Datas/clicks/clicks_hour_305.csv\n",
      "Loading: Datas/clicks/clicks_hour_306.csv\n",
      "Loading: Datas/clicks/clicks_hour_307.csv\n",
      "Loading: Datas/clicks/clicks_hour_308.csv\n",
      "Loading: Datas/clicks/clicks_hour_309.csv\n",
      "Loading: Datas/clicks/clicks_hour_310.csv\n",
      "Loading: Datas/clicks/clicks_hour_311.csv\n",
      "Loading: Datas/clicks/clicks_hour_312.csv\n",
      "Loading: Datas/clicks/clicks_hour_313.csv\n",
      "Loading: Datas/clicks/clicks_hour_314.csv\n",
      "Loading: Datas/clicks/clicks_hour_315.csv\n",
      "Loading: Datas/clicks/clicks_hour_316.csv\n",
      "Loading: Datas/clicks/clicks_hour_317.csv\n",
      "Loading: Datas/clicks/clicks_hour_318.csv\n",
      "Loading: Datas/clicks/clicks_hour_319.csv\n",
      "Loading: Datas/clicks/clicks_hour_320.csv\n",
      "Loading: Datas/clicks/clicks_hour_321.csv\n",
      "Loading: Datas/clicks/clicks_hour_322.csv\n",
      "Loading: Datas/clicks/clicks_hour_323.csv\n",
      "Loading: Datas/clicks/clicks_hour_324.csv\n",
      "Loading: Datas/clicks/clicks_hour_325.csv\n",
      "Loading: Datas/clicks/clicks_hour_326.csv\n",
      "Loading: Datas/clicks/clicks_hour_327.csv\n",
      "Loading: Datas/clicks/clicks_hour_328.csv\n",
      "Loading: Datas/clicks/clicks_hour_329.csv\n",
      "Loading: Datas/clicks/clicks_hour_330.csv\n",
      "Loading: Datas/clicks/clicks_hour_331.csv\n",
      "Loading: Datas/clicks/clicks_hour_332.csv\n",
      "Loading: Datas/clicks/clicks_hour_333.csv\n",
      "Loading: Datas/clicks/clicks_hour_334.csv\n",
      "Loading: Datas/clicks/clicks_hour_335.csv\n",
      "Loading: Datas/clicks/clicks_hour_336.csv\n",
      "Loading: Datas/clicks/clicks_hour_337.csv\n",
      "Loading: Datas/clicks/clicks_hour_338.csv\n",
      "Loading: Datas/clicks/clicks_hour_339.csv\n",
      "Loading: Datas/clicks/clicks_hour_340.csv\n",
      "Loading: Datas/clicks/clicks_hour_341.csv\n",
      "Loading: Datas/clicks/clicks_hour_342.csv\n",
      "Loading: Datas/clicks/clicks_hour_343.csv\n",
      "Loading: Datas/clicks/clicks_hour_344.csv\n",
      "Loading: Datas/clicks/clicks_hour_345.csv\n",
      "Loading: Datas/clicks/clicks_hour_346.csv\n",
      "Loading: Datas/clicks/clicks_hour_347.csv\n",
      "Loading: Datas/clicks/clicks_hour_348.csv\n",
      "Loading: Datas/clicks/clicks_hour_349.csv\n",
      "Loading: Datas/clicks/clicks_hour_350.csv\n",
      "Loading: Datas/clicks/clicks_hour_351.csv\n",
      "Loading: Datas/clicks/clicks_hour_352.csv\n",
      "Loading: Datas/clicks/clicks_hour_353.csv\n",
      "Loading: Datas/clicks/clicks_hour_354.csv\n",
      "Loading: Datas/clicks/clicks_hour_355.csv\n",
      "Loading: Datas/clicks/clicks_hour_356.csv\n",
      "Loading: Datas/clicks/clicks_hour_357.csv\n",
      "Loading: Datas/clicks/clicks_hour_358.csv\n",
      "Loading: Datas/clicks/clicks_hour_359.csv\n",
      "Loading: Datas/clicks/clicks_hour_360.csv\n",
      "Loading: Datas/clicks/clicks_hour_361.csv\n",
      "Loading: Datas/clicks/clicks_hour_362.csv\n",
      "Loading: Datas/clicks/clicks_hour_363.csv\n",
      "Loading: Datas/clicks/clicks_hour_364.csv\n",
      "Loading: Datas/clicks/clicks_hour_365.csv\n",
      "Loading: Datas/clicks/clicks_hour_366.csv\n",
      "Loading: Datas/clicks/clicks_hour_367.csv\n",
      "Loading: Datas/clicks/clicks_hour_368.csv\n",
      "Loading: Datas/clicks/clicks_hour_369.csv\n",
      "Loading: Datas/clicks/clicks_hour_370.csv\n",
      "Loading: Datas/clicks/clicks_hour_371.csv\n",
      "Loading: Datas/clicks/clicks_hour_372.csv\n",
      "Loading: Datas/clicks/clicks_hour_373.csv\n",
      "Loading: Datas/clicks/clicks_hour_374.csv\n",
      "Loading: Datas/clicks/clicks_hour_375.csv\n",
      "Loading: Datas/clicks/clicks_hour_376.csv\n",
      "Loading: Datas/clicks/clicks_hour_377.csv\n",
      "Loading: Datas/clicks/clicks_hour_378.csv\n",
      "Loading: Datas/clicks/clicks_hour_379.csv\n",
      "Loading: Datas/clicks/clicks_hour_380.csv\n",
      "Loading: Datas/clicks/clicks_hour_381.csv\n",
      "Loading: Datas/clicks/clicks_hour_382.csv\n",
      "Loading: Datas/clicks/clicks_hour_383.csv\n",
      "Loading: Datas/clicks/clicks_hour_384.csv\n",
      "Combined 385 click files\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "if debug:\n",
    "    clicks_df = pd.read_csv(CLICKS_SAMPLE_PATH)\n",
    "else:\n",
    "    clicks_files = []\n",
    "        \n",
    "    # Check if CLICKS_PATH is a directory or file\n",
    "    if os.path.isdir(CLICKS__PATH):\n",
    "        # Load all CSV files in the directory\n",
    "        for file in os.listdir(CLICKS__PATH):\n",
    "            if file.endswith('.csv'):\n",
    "                file_path = os.path.join(CLICKS__PATH, file)\n",
    "                print(f\"Loading: {file_path}\")\n",
    "                clicks_files.append(pd.read_csv(file_path))\n",
    "        \n",
    "\n",
    "        clicks_df = pd.concat(clicks_files, ignore_index=True)\n",
    "        print(f\"Combined {len(clicks_files)} click files\")\n",
    "                                   \n",
    "#rename click_article_df for merge\n",
    "clicks_df.rename(columns={'click_article_id':'article_id'}, errors=\"raise\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7fcc0805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clicks shape: (2988181, 12)\n",
      "Number of interactions: 2988181\n",
      "Number of unique users: 322897\n",
      "Number of unique articles: 46033\n",
      "Date range: 1506826800026 to 1510603454886\n",
      "\n",
      "First few rows of clicks data:\n",
      "  user_id        session_id  ... click_region click_referrer_type\n",
      "0       0  1506825423271737  ...           20                   2\n",
      "1       0  1506825423271737  ...           20                   2\n",
      "2       1  1506825426267738  ...           16                   2\n",
      "3       1  1506825426267738  ...           16                   2\n",
      "4       2  1506825435299739  ...           24                   2\n",
      "\n",
      "[5 rows x 12 columns]\n",
      "\n",
      " MISSING VALUES:\n",
      "user_id                0\n",
      "session_id             0\n",
      "session_start          0\n",
      "session_size           0\n",
      "article_id             0\n",
      "click_timestamp        0\n",
      "click_environment      0\n",
      "click_deviceGroup      0\n",
      "click_os               0\n",
      "click_country          0\n",
      "click_region           0\n",
      "click_referrer_type    0\n",
      "dtype: int64\n",
      "\n",
      " DUPLICATES:\n",
      "0\n",
      "\n",
      " DATA TYPES:\n",
      "user_id                object\n",
      "session_id             object\n",
      "session_start          object\n",
      "session_size           object\n",
      "article_id             object\n",
      "click_timestamp        object\n",
      "click_environment      object\n",
      "click_deviceGroup      object\n",
      "click_os               object\n",
      "click_country          object\n",
      "click_region           object\n",
      "click_referrer_type    object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(f\"Clicks shape: {clicks_df.shape}\")\n",
    "\n",
    "print(f\"Number of interactions: {len(clicks_df)}\")\n",
    "print(f\"Number of unique users: {clicks_df['user_id'].nunique()}\")\n",
    "print(f\"Number of unique articles: {clicks_df['article_id'].nunique()}\")\n",
    "\n",
    "print(f\"Date range: {clicks_df['click_timestamp'].min()} to {clicks_df['click_timestamp'].max()}\")\n",
    "\n",
    "print(\"\\nFirst few rows of clicks data:\")\n",
    "print(clicks_df.head())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\n MISSING VALUES:\")\n",
    "print(clicks_df.isnull().sum())\n",
    "\n",
    "# Check for duplicates\n",
    "print(f\"\\n DUPLICATES:\")\n",
    "print(clicks_df.duplicated().sum())\n",
    "\n",
    "# Check data types\n",
    "print(\"\\n DATA TYPES:\")\n",
    "print(clicks_df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16384c67",
   "metadata": {},
   "source": [
    "clicks_{}.csv contient 12 collonnes :\n",
    "\n",
    "    user_id : user ID\n",
    "    session_id : Session ID\n",
    "    session_start : Début de session (timestamp)\n",
    "    session_size : nombre d'article vu sur la session\n",
    "    click_article_id : article ID user clicked\n",
    "    click_timestamp : When user clicked (timestamp)\n",
    "    click_environment : user env\n",
    "    click_deviceGroup : user device\n",
    "    click_os : user OS\n",
    "    click_country : localisation (country)\n",
    "    click_region : localisation (region)\n",
    "    click_referrer_type : ?\n",
    "\n",
    "On peut drop \n",
    "    click_environment      \n",
    "    click_deviceGroup      \n",
    "    click_os               \n",
    "    click_country          \n",
    "    click_region           \n",
    "    click_referrer_type    \n",
    "\n",
    "On peut merge les metadatas conservées\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1d8067",
   "metadata": {},
   "source": [
    "On récupère la différence entre le précédant click et le clic actuel pour avoir le temps assé sur chaque article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f724b6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "clicks_df = clicks_df.apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff67479a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "time_spend_on_article",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "75fea474-6b54-4673-8b50-2aee43b81a40",
       "rows": [
        [
         "count",
         "2988181.0"
        ],
        [
         "mean",
         "569876.0059089459"
        ],
        [
         "std",
         "5442533.483914518"
        ],
        [
         "min",
         "0.0"
        ],
        [
         "25%",
         "30000.0"
        ],
        [
         "50%",
         "39903.0"
        ],
        [
         "75%",
         "202575.0"
        ],
        [
         "max",
         "1212149256.0"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 8
       }
      },
      "text/plain": [
       "count       2,988,181.00\n",
       "mean          569,876.01\n",
       "std         5,442,533.48\n",
       "min                 0.00\n",
       "25%            30,000.00\n",
       "50%            39,903.00\n",
       "75%           202,575.00\n",
       "max     1,212,149,256.00\n",
       "Name: time_spend_on_article, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clicks_df['prev_click_timestamp'] = clicks_df.groupby('session_id')['click_timestamp'].shift(1)\n",
    "clicks_df.loc[clicks_df['prev_click_timestamp'].isna(),'prev_click_timestamp'] = clicks_df['session_start']\n",
    "clicks_df['time_spend_on_article'] = clicks_df['click_timestamp']-clicks_df['prev_click_timestamp']\n",
    "\n",
    "clicks_df['time_spend_on_article'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efe37a2",
   "metadata": {},
   "source": [
    "On comprend qu'un timestamp automatique à +30000 a été appliqué à toutes les dernières entrées de session, pour lesquelles on a pas de timestamp de sortie.\n",
    "\n",
    "On peut soit essayer de corriger ça en remplaçant par exemple par la durée moyenne de lecture pour une meilleur approximation, soit abandonner la pondération par temps de lecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91180752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temps moyen passé sur un article hors inférence 861752.947533867\n",
      "temps median passé sur un article hors inférence 130404.0\n"
     ]
    }
   ],
   "source": [
    "mean_timespend = clicks_df.loc[clicks_df['time_spend_on_article']!=30000,'time_spend_on_article'].mean()\n",
    "med_timespend = clicks_df.loc[clicks_df['time_spend_on_article']!=30000,'time_spend_on_article'].median()\n",
    "\n",
    "print(f'temps moyen passé sur un article hors inférence {mean_timespend}')\n",
    "print(f'temps median passé sur un article hors inférence {med_timespend}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8cbb5591",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clicks_df.loc[clicks_df['time_spend_on_article']==30000,'time_spend_on_article']=med_timespend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "72ba97f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "clicks_df.drop(columns = [\n",
    "                'session_id',\n",
    "                'session_size',\n",
    "                # 'click_environment',\n",
    "                # 'click_deviceGroup',\n",
    "                # 'click_os',\n",
    "                # 'click_country',\n",
    "                # 'click_region',\n",
    "                # 'click_referrer_type',\n",
    "                \n",
    "                'prev_click_timestamp'],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7560f61a",
   "metadata": {},
   "source": [
    "On va avoir besoind e récuperer certaines metadata des articles pour créer notre rating. On merge donc les DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e7e6b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged shape: (2988181, 15)\n"
     ]
    }
   ],
   "source": [
    "# Merge datasets\n",
    "clicks_df = clicks_df.merge(\n",
    "    articles_df, \n",
    "    on='article_id', \n",
    "    how='left'\n",
    ")\n",
    "print(f\"Merged shape: {clicks_df.shape}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01f475c",
   "metadata": {},
   "source": [
    "#### Rating\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "92278184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "words_count",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "a71d9953-21fa-423c-b0c1-579514f3e86e",
       "rows": [
        [
         "count",
         "2988181.0"
        ],
        [
         "mean",
         "208.62833810937155"
        ],
        [
         "std",
         "81.60152023441293"
        ],
        [
         "min",
         "0.0"
        ],
        [
         "25%",
         "173.0"
        ],
        [
         "50%",
         "198.0"
        ],
        [
         "75%",
         "232.0"
        ],
        [
         "max",
         "6690.0"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 8
       }
      },
      "text/plain": [
       "count   2,988,181.00\n",
       "mean          208.63\n",
       "std            81.60\n",
       "min             0.00\n",
       "25%           173.00\n",
       "50%           198.00\n",
       "75%           232.00\n",
       "max         6,690.00\n",
       "Name: words_count, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clicks_df['words_count'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2fac9c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "clicks_df.loc[clicks_df['words_count']<=0,'words_count']=198"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "02b7a4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "clicks_df['time_per_word'] = clicks_df['time_spend_on_article']/clicks_df['words_count']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829f1965",
   "metadata": {},
   "source": [
    "## C. articles_embeding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e7c894",
   "metadata": {},
   "source": [
    "- Le pickle article_embedding est une représentation abstraites des articles qui vas nous servir pour la recommendation:\n",
    "    Pickle (Python 3) of a NumPy matrix containing the Article Content Embeddings (250-dimensional vectors), trained upon articles' text and metadata by the CHAMELEON's ACR module (see paper for details) for 364047 published articles.\n",
    "    P.s. The full text of news articles could not be provided due to license restrictions, but those embeddings can be used by Neural Networks to represent their content. See this paper for a t-SNE visualization of these embeddings, colored by category.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e49fef28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeding shape: (364047, 250)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open(EMBEDING_PATH , 'rb') as f:    \n",
    "    embeding_df = pickle.load(f)\n",
    "\n",
    "embeding_df = pd.DataFrame(embeding_df)\n",
    "\n",
    "\n",
    "print(f\"Embeding shape: {embeding_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4047218c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings_with_id = embeding_df.reset_index().rename(columns={'index': 'article_id'})\n",
    "embeding_df.index.names = ['article_id']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657fcce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Merge datasets\n",
    "# merged_df = merged_df.merge(\n",
    "#     embeding_df, \n",
    "#     on='article_id', \n",
    "#     how='left'\n",
    "# )\n",
    "# print(f\"Merged shape: {merged_df.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e866a9c",
   "metadata": {},
   "source": [
    " la data ne semble pas fiable pour determiner le temps passé sur els article:\n",
    "    Sur les session de 2 article la difference de click_timestamp est toujours identique.\n",
    "    il n'existe aucune session de 1 article\n",
    "    l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ef972c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clicks_df.to_pickle(RATING_PREPROC_DF_PATH)\n",
    "embeding_df.to_pickle(CANDIDATE_PREPROC_DF_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d435bf7",
   "metadata": {},
   "source": [
    "# II. Model de recommandation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81a4075f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(RATING_PREPROC_DF_PATH , 'rb') as f:\n",
    "    rating_df = pickle.load(f)\n",
    "with open(CANDIDATE_PREPROC_DF_PATH , 'rb') as f:\n",
    "    candidate_df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9638556",
   "metadata": {},
   "source": [
    "##  librairie: implicit vs surprise vs tensorflow reocmmendation:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea2fbff",
   "metadata": {},
   "source": [
    "Implicit n'est pas en V1 et n'a pas vu de commit depuis plus d'un an. Il n'y a pas de documentation hormis  le git.\n",
    "\n",
    "Surprise est mieux maintenu  et plus cité mais plus orienté explicit qu'implicit\n",
    "\n",
    "tfrs est ~~bien maintenu~~(non), addossé a tensorflow, et adapté au problème:\n",
    "https://www.tensorflow.org/recommenders/examples/basic_retrieval\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd549e9",
   "metadata": {},
   "source": [
    "## Popularity (Dummy)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5dc2ca5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from popularity_model import PopularityArticleRecommender"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a03038",
   "metadata": {},
   "source": [
    "## A. IMPLICIT (for explicit/implicit collaborative filtering)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc02a1d7",
   "metadata": {},
   "source": [
    "### 1. Model architecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3f40330",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\d0t\\anaconda3\\envs\\projet3\\Lib\\site-packages\\mlflow\\pyfunc\\utils\\data_validation.py:186: UserWarning: \u001b[33mAdd type hints to the `predict` method to enable data validation and automatic signature inference during model logging. Check https://mlflow.org/docs/latest/model/python_model.html#type-hint-usage-in-pythonmodel for more details.\u001b[0m\n",
      "  color_warning(\n"
     ]
    }
   ],
   "source": [
    "from implicit_model import ArticleRetrievalImplicit\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ba1165",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "retrieval_model = ArticleRetrievalImplicit(rating_df, \n",
    "                                           candidate_df, \n",
    "                                           factors=64,\n",
    "                                           model_type='BAY'\n",
    "                                           )\n",
    "retrieval_model.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dea37b1",
   "metadata": {},
   "source": [
    "### 2. Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e8524e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from implicit_model import implicit_evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b13dd5",
   "metadata": {},
   "source": [
    "#### test de recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c7b01847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'article_id': '331242', 'score': 7.332470417022705}, {'article_id': '331664', 'score': 6.8674774169921875}, {'article_id': '10253', 'score': 6.760776042938232}, {'article_id': '353786', 'score': 6.688801288604736}, {'article_id': '36160', 'score': 6.6756181716918945}]\n"
     ]
    }
   ],
   "source": [
    "# Recommend articles for a user\n",
    "result = retrieval_model.recommend(\"8\", N=5)\n",
    "print(result)\n",
    "\n",
    "# Find similar articles\n",
    "# print(retrieval_model.similar_items('42', N=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b9cfd5",
   "metadata": {},
   "source": [
    "### 3. Entrainement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361c72c8",
   "metadata": {},
   "source": [
    "#### Initialisation mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69f81e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlflow server --host 127.0.0.1  --port 8080 \n",
      "\n",
      "                mlflow ui --backend-store-uri /mlruns\n"
     ]
    }
   ],
   "source": [
    "from mlflow_tools import start_local_experiment\n",
    "start_local_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc733a70",
   "metadata": {},
   "source": [
    "#### Experimentation mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c9432c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow_tools import mlflow_experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c4f228",
   "metadata": {},
   "source": [
    "##### Dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d263444",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_params = {\n",
    "    'rating_df' : rating_df,\n",
    "    'candidate_df' : candidate_df,\n",
    "    'ratings_keep' : [\"user_id\",\"article_id\",\"time_per_word\",\"category_id\"],\n",
    "    'candidates_keep' : [\"article_id\"],                      \n",
    "    'rating_target' : \"time_per_word\",\n",
    "    'train_test_split_perc' : 0.8,\n",
    "    'model_class': PopularityArticleRecommender,\n",
    "    'evaluation_type' : 'CB' ,  \n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3056f4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_params = {\n",
    "    'rating_df' : rating_df,\n",
    "    'candidate_df' : candidate_df,\n",
    "    'ratings_keep' : [\"user_id\",\"article_id\",\"time_per_word\",\"category_id\"],\n",
    "    'candidates_keep' : [\"article_id\"],                      \n",
    "    'rating_target' : None,\n",
    "    'train_test_split_perc' : 0.8,\n",
    "    'model_class': PopularityArticleRecommender,\n",
    "    'evaluation_type' : 'CB' ,  \n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599957e5",
   "metadata": {},
   "source": [
    "##### Reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b3d6158",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_params = {\n",
    "    'rating_df' : rating_df,\n",
    "    'candidate_df' : candidate_df,\n",
    "    'ratings_keep' : [\"user_id\",\"article_id\",\"time_per_word\",\"category_id\"],\n",
    "    'candidates_keep' : [\"article_id\"],                      \n",
    "    'rating_target' : [\"time_per_word\",\"category_id\"],\n",
    "    'train_test_split_perc' : 0.8,\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb06c0e6",
   "metadata": {},
   "source": [
    "##### ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e5f60ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_params = {\n",
    "    'rating_df' : rating_df,\n",
    "    'candidate_df' : candidate_df,\n",
    "    'ratings_keep' : [\"user_id\",\"article_id\",\"time_per_word\",\"category_id\"],\n",
    "    'candidates_keep' : [\"article_id\"],                      \n",
    "    'rating_target' : None,\n",
    "    'train_test_split_perc' : 0.8,\n",
    "    'model_type':'ALS'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12d88129",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_params = {\n",
    "    'rating_df' : rating_df,\n",
    "    'candidate_df' : candidate_df,\n",
    "    'ratings_keep' : [\"user_id\",\"article_id\",\"time_per_word\"],\n",
    "    'candidates_keep' : [\"article_id\"],                      \n",
    "    'rating_target' : 'time_per_word',\n",
    "    'train_test_split_perc' : 0.8,\n",
    "    'model_type':'ALS'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630fa691",
   "metadata": {},
   "source": [
    "##### Bayesian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aba50d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_params = {\n",
    "    'rating_df' : rating_df,\n",
    "    'candidate_df' : candidate_df,\n",
    "    'ratings_keep' : [\"user_id\",\"article_id\",\"time_per_word\"],\n",
    "    'candidates_keep' : [\"article_id\"],                      \n",
    "    'rating_target' : \"time_per_word\",\n",
    "    'train_test_split_perc' : 0.8,\n",
    "    'model_type':'BAY'\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "566f6fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_params = {\n",
    "    'rating_df' : rating_df,\n",
    "    'candidate_df' : candidate_df,\n",
    "    'ratings_keep' : [\"user_id\",\"article_id\",\"category_id\"],\n",
    "    'candidates_keep' : [\"article_id\"],                      \n",
    "    'rating_target' : None,\n",
    "    'train_test_split_perc' : 0.8,\n",
    "    'model_type':'BAY'\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c4860cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_params = {\n",
    "    'rating_df' : rating_df,\n",
    "    'candidate_df' : candidate_df,\n",
    "    'ratings_keep' : [\"user_id\",\"article_id\",\"category_id\",'time_spend_on_article','words_count'],\n",
    "    'candidates_keep' : [\"article_id\"],                      \n",
    "    'rating_target' : None,\n",
    "    'train_test_split_perc' : 0.8,\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5249eb1",
   "metadata": {},
   "source": [
    "##### User metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bbae1843",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_params = {\n",
    "    'rating_df' : rating_df,\n",
    "    'candidate_df' : candidate_df,\n",
    "    'ratings_keep' : [\"user_id\",\"article_id\",\"category_id\",'time_spend_on_article','words_count',\n",
    "                      'click_environment','click_deviceGroup','click_country','click_region'],\n",
    "    'candidates_keep' : [\"article_id\"],                      \n",
    "    'rating_target' : None,\n",
    "    'train_test_split_perc' : 0.8,\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41704964",
   "metadata": {},
   "source": [
    "##### Article embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4932e453",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_params = {\n",
    "    'rating_df' : rating_df,\n",
    "    'candidate_df' : candidate_df,\n",
    "    'ratings_keep' : [\"user_id\",\"article_id\",\"time_per_word\",\"category_id\"],\n",
    "    'candidates_keep' : [\"article_id\"],                      \n",
    "    'rating_target' : None,\n",
    "    'train_test_split_perc' : 0.8,\n",
    "    'add_embeding_vector': True,\n",
    "    'embeding_alpha':0.9\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bcb88fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_params = {\n",
    "    'rating_df' : rating_df,\n",
    "    'candidate_df' : candidate_df,\n",
    "    'ratings_keep' : [\"user_id\",\"article_id\",\"time_per_word\",\"category_id\"],\n",
    "    'candidates_keep' : [\"article_id\"],                      \n",
    "    'rating_target' : None,\n",
    "    'train_test_split_perc' : 0.8,\n",
    "    'add_embeding_vector': True,\n",
    "    'use_pca' : True,\n",
    "    'embeding_alpha':0.9\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "629bf8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_params = {\n",
    "    'rating_df' : rating_df,\n",
    "    'candidate_df' : candidate_df,\n",
    "    'ratings_keep' : [\"user_id\",\"article_id\",\"time_per_word\",\"category_id\"],\n",
    "    'candidates_keep' : [\"article_id\"],                      \n",
    "    'rating_target' : None,\n",
    "    'train_test_split_perc' : 0.8,\n",
    "    'add_embeding_vector': True,\n",
    "    'use_pca' : True,\n",
    "    'embeding_alpha':0.5\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f50bfa32",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_params = {\n",
    "    'rating_df' : rating_df,\n",
    "    'candidate_df' : candidate_df,\n",
    "    'ratings_keep' : [\"user_id\",\"article_id\",\"time_per_word\",\"category_id\"],\n",
    "    'candidates_keep' : [\"article_id\"],                      \n",
    "    'rating_target' : None,\n",
    "    'train_test_split_perc' : 0.8,\n",
    "    'add_embeding_vector': False,\n",
    "    'use_pca' : True,\n",
    "    'embeding_alpha':0\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "849c500f",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_params = {\n",
    "    'rating_df' : rating_df,\n",
    "    'candidate_df' : candidate_df,\n",
    "    'ratings_keep' : [\"user_id\",\"article_id\",\"time_per_word\",\"category_id\"],\n",
    "    'candidates_keep' : [\"article_id\"],                      \n",
    "    'rating_target' : [\"time_per_word\",\"category_id\"],\n",
    "    'train_test_split_perc' : 0.8,\n",
    "    'add_embeding_vector': True,\n",
    "    'use_pca' : True,\n",
    "    'embeding_alpha':0,\n",
    "    'embedding_type' : 'USER_MEAN',\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75f0c163",
   "metadata": {},
   "outputs": [],
   "source": [
    "from recommenders_models import ArticleRetrievalRecommenders as ARR_recommenders\n",
    "experiment_params = {\n",
    "    'rating_df' : rating_df,\n",
    "    'candidate_df' : candidate_df,\n",
    "    'ratings_keep' : [\"user_id\",\"article_id\",\"time_per_word\",\"category_id\"],\n",
    "    'candidates_keep' : [\"article_id\"],                      \n",
    "    'rating_target' : [\"time_per_word\",\"category_id\"],\n",
    "    'train_test_split_perc' : 0.8,\n",
    "    'add_embeding_vector': True,\n",
    "    'use_pca' : True,\n",
    "    'embeding_alpha':0,\n",
    "    'embedding_type' : 'USER_MEAN',\n",
    "    'model_class' : ARR_recommenders,\n",
    "    'model_type' : 'SAR'\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a19a8c8",
   "metadata": {},
   "source": [
    "##### ContentBased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f2725b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\d0t\\anaconda3\\envs\\projet3\\Lib\\site-packages\\mlflow\\pyfunc\\utils\\data_validation.py:186: UserWarning: \u001b[33mAdd type hints to the `predict` method to enable data validation and automatic signature inference during model logging. Check https://mlflow.org/docs/latest/model/python_model.html#type-hint-usage-in-pythonmodel for more details.\u001b[0m\n",
      "  color_warning(\n"
     ]
    }
   ],
   "source": [
    "from content_base_model import ContentBasedRecommender\n",
    "experiment_params = {\n",
    "    'rating_df' : rating_df,\n",
    "    'candidate_df' : candidate_df,\n",
    "    #'sample': 1000,\n",
    "    'ratings_keep' : [\"user_id\",\"article_id\",\"time_per_word\",\"category_id\"],\n",
    "    'candidates_keep' : [\"article_id\"],                      \n",
    "    'rating_target' : [],\n",
    "    'train_test_split_perc' : 0.8,\n",
    "    'model_class' : ContentBasedRecommender,\n",
    "    'evaluation_type' : 'CB' ,  \n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7f2ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from content_base_model import ContentBasedRecommender\n",
    "experiment_params = {\n",
    "    'rating_df' : rating_df,\n",
    "    'candidate_df' : candidate_df,\n",
    "    #'sample': 1000,\n",
    "    'ratings_keep' : [\"user_id\",\"article_id\",\"time_per_word\",\"category_id\"],\n",
    "    'candidates_keep' : [\"article_id\"],                      \n",
    "    'rating_target' : [\"time_per_word\"],\n",
    "    'train_test_split_perc' : 0.8,\n",
    "    'model_class' : ContentBasedRecommender,\n",
    "    'evaluation_type' : 'CB' ,  \n",
    "\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec705281",
   "metadata": {},
   "outputs": [],
   "source": [
    "from content_base_model import ContentBasedRecommender\n",
    "experiment_params = {\n",
    "    'rating_df' : rating_df,\n",
    "    'candidate_df' : candidate_df,\n",
    "    #'sample': 1000,\n",
    "    'ratings_keep' : [\"user_id\",\"article_id\",\"time_per_word\",\"category_id\"],\n",
    "    'candidates_keep' : [\"article_id\"],                      \n",
    "    'rating_target' : [\"time_per_word\"],\n",
    "    'train_test_split_perc' : 0.8,\n",
    "    'model_class' : ContentBasedRecommender,\n",
    "    'evaluation_type' : 'CB' ,  \n",
    "    'seen_candidate_only' : True,\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc69012c",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_params = {\n",
    "    'rating_df' : rating_df,\n",
    "    'candidate_df' : candidate_df,\n",
    "    'sample': 1000,\n",
    "    'ratings_keep' : [\"user_id\",\"article_id\",\"time_per_word\",\"category_id\"],\n",
    "    'candidates_keep' : [\"article_id\"],                      \n",
    "    'rating_target' : [\"time_per_word\"],\n",
    "    'train_test_split_perc' : 0.8,\n",
    "    'model_class' : ContentBasedRecommender,\n",
    "    'evaluation_type' : 'CB' ,  \n",
    "    'seen_candidate_only' : True,\n",
    "    \n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a01f44c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_params = {\n",
    "    'rating_df' : rating_df,\n",
    "    'candidate_df' : candidate_df,\n",
    "    'ratings_keep' : [\"user_id\",\"article_id\",\"time_per_word\",\"category_id\"],\n",
    "    'candidates_keep' : [\"article_id\"],                      \n",
    "    'rating_target' : [],\n",
    "    'train_test_split_perc' : 0.8,\n",
    "    'model_class' : ContentBasedRecommender,\n",
    "    'evaluation_type' : 'CB' ,  \n",
    "    'seen_candidate_only' : True,\n",
    "    \n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef902c99",
   "metadata": {},
   "source": [
    "##### Launch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f14d484a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training BAY model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:05<00:00,  8.86it/s, train_auc=98.51%, skipped=4.02%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After fit:\n",
      "Training complete.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210253/210253 [00:19<00:00, 10898.68it/s]\n",
      "100%|██████████| 210253/210253 [00:18<00:00, 11069.23it/s]\n",
      "100%|██████████| 210253/210253 [00:18<00:00, 11369.14it/s]\n",
      "100%|██████████| 210253/210253 [00:18<00:00, 11263.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run grandiose-duck-222 at: http://127.0.0.1:8080/#/experiments/362647451172403616/runs/885797c9f5f94cfda81bcdb9a8079dd0\n",
      "🧪 View experiment at: http://127.0.0.1:8080/#/experiments/362647451172403616\n"
     ]
    }
   ],
   "source": [
    "print('')\n",
    "model = mlflow_experiment(**experiment_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125db8e4",
   "metadata": {},
   "source": [
    "### 5. Precomputing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc995f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from implicit_model import precompute_model_results\n",
    "\n",
    "MODEL_PATH = 'model_assets/model.pkl'\n",
    "with open(MODEL_PATH, 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "precompute_dic = precompute_model_results(model,N=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1fd0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = precompute_dic['100000']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70044454",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model_assets/precompute_dic.pkl', 'wb') as file:\n",
    "    pickle.dump(precompute_dic, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c977c19",
   "metadata": {},
   "source": [
    "## B- Content-based"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62e649a",
   "metadata": {},
   "source": [
    "### 1. model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "676057ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from content_base_model import ContentBasedRecommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b64ac780",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ContentBasedRecommender(rating_df,candidate_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b25e72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f25478a",
   "metadata": {},
   "source": [
    "12 minute with slow python loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fa2027",
   "metadata": {},
   "source": [
    "12min python loop, 1min 30 vectorized, 12s FAISS san precomp, 2m18s precompute 105, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a27036bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Index([157536, 157122, 161915, 105687, 155573, 149603, 162026, 160453, 156623,\n",
       "        267015],\n",
       "       dtype='int64'),\n",
       " array([0.7607803 , 0.757375  , 0.7505505 , 0.74798596, 0.74087423,\n",
       "        0.7383653 , 0.7380524 , 0.73552585, 0.7344037 , 0.7330377 ],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.faiss_recommend_items(100011)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bcdf7c36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Index([157728, 153421, 284362, 150436, 155662, 160711, 157000, 159539, 289045,\n",
       "        346062],\n",
       "       dtype='int64'),\n",
       " array([0.80296916, 0.7828106 , 0.7755932 , 0.7748253 , 0.76989317,\n",
       "        0.7691962 , 0.76218307, 0.76191413, 0.7615298 , 0.7610241 ],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.faiss_recommend_items(100011)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66a12bf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "article_id",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "recStrength",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "50843f0c-0565-4591-82a0-d90eb5a52ed0",
       "rows": [
        [
         "0",
         "157728.0",
         "0.8029688596725464"
        ],
        [
         "1",
         "153421.0",
         "0.7828106880187988"
        ],
        [
         "2",
         "284362.0",
         "0.775593101978302"
        ],
        [
         "3",
         "150436.0",
         "0.774824857711792"
        ],
        [
         "4",
         "155662.0",
         "0.7698933482170105"
        ],
        [
         "5",
         "160711.0",
         "0.769196093082428"
        ],
        [
         "6",
         "157000.0",
         "0.762182891368866"
        ],
        [
         "7",
         "159539.0",
         "0.7619145512580872"
        ],
        [
         "8",
         "289045.0",
         "0.7615296840667725"
        ],
        [
         "9",
         "346062.0",
         "0.761023759841919"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>recStrength</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>157728.0</td>\n",
       "      <td>0.802969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>153421.0</td>\n",
       "      <td>0.782811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>284362.0</td>\n",
       "      <td>0.775593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>150436.0</td>\n",
       "      <td>0.774825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>155662.0</td>\n",
       "      <td>0.769893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>160711.0</td>\n",
       "      <td>0.769196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>157000.0</td>\n",
       "      <td>0.762183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>159539.0</td>\n",
       "      <td>0.761915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>289045.0</td>\n",
       "      <td>0.761530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>346062.0</td>\n",
       "      <td>0.761024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_id  recStrength\n",
       "0    157728.0     0.802969\n",
       "1    153421.0     0.782811\n",
       "2    284362.0     0.775593\n",
       "3    150436.0     0.774825\n",
       "4    155662.0     0.769893\n",
       "5    160711.0     0.769196\n",
       "6    157000.0     0.762183\n",
       "7    159539.0     0.761915\n",
       "8    289045.0     0.761530\n",
       "9    346062.0     0.761024"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.recommend_items(100011)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e001239",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "article_id",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "recStrength",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "0598da05-0def-4b1f-91e0-619a0c64e3d3",
       "rows": [
        [
         "0",
         "157728.0",
         "0.8029689986784609"
        ],
        [
         "1",
         "153421.0",
         "0.7828106374017207"
        ],
        [
         "2",
         "284362.0",
         "0.7755931143996302"
        ],
        [
         "3",
         "150436.0",
         "0.7748249214328663"
        ],
        [
         "4",
         "155662.0",
         "0.769893334796543"
        ],
        [
         "5",
         "160711.0",
         "0.7691961411862019"
        ],
        [
         "6",
         "157000.0",
         "0.7621828478527802"
        ],
        [
         "7",
         "159539.0",
         "0.7619145352851205"
        ],
        [
         "8",
         "289045.0",
         "0.7615297657019718"
        ],
        [
         "9",
         "346062.0",
         "0.7610236555614966"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>recStrength</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>157728.0</td>\n",
       "      <td>0.802969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>153421.0</td>\n",
       "      <td>0.782811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>284362.0</td>\n",
       "      <td>0.775593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>150436.0</td>\n",
       "      <td>0.774825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>155662.0</td>\n",
       "      <td>0.769893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>160711.0</td>\n",
       "      <td>0.769196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>157000.0</td>\n",
       "      <td>0.762183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>159539.0</td>\n",
       "      <td>0.761915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>289045.0</td>\n",
       "      <td>0.761530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>346062.0</td>\n",
       "      <td>0.761024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_id  recStrength\n",
       "0    157728.0     0.802969\n",
       "1    153421.0     0.782811\n",
       "2    284362.0     0.775593\n",
       "3    150436.0     0.774825\n",
       "4    155662.0     0.769893\n",
       "5    160711.0     0.769196\n",
       "6    157000.0     0.762183\n",
       "7    159539.0     0.761915\n",
       "8    289045.0     0.761530\n",
       "9    346062.0     0.761024"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.recommend_items(100011)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "744962dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "CB_MODEL_PATH = 'model_assets/cf_model.pkl'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "78ba3c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(CB_MODEL_PATH, 'wb') as file:\n",
    "    pickle.dump(model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c60cd9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(CB_MODEL_PATH, 'rb') as f:\n",
    "    model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e231226f",
   "metadata": {},
   "source": [
    "### 2. Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c29cdc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_evaluator import RecommendationModelEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41285830",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_evaluator = RecommendationModelEvaluator(model)\n",
    "\n",
    "print('Evaluating Content-Based Filtering model...')\n",
    "cb_global_metrics, cb_detailed_results_df = model_evaluator.evaluate_model(model)\n",
    "print('\\nGlobal metrics:\\n%s' % cb_global_metrics)\n",
    "cb_detailed_results_df.head(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db8ca1a",
   "metadata": {},
   "source": [
    "## C. Keras+tfrs (abandonné car mal maintenu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b362b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Sequential,layers,Model\n",
    "\n",
    "import tensorflow_recommenders as tfrs\n",
    "\n",
    "\n",
    "# Build vocabularies\n",
    "user_ids_vocabulary = layers.StringLookup(\n",
    "    vocabulary=rating_df[\"user_id\"].astype(str).unique(), mask_token=None\n",
    ")\n",
    "article_ids_vocabulary = layers.StringLookup(\n",
    "    vocabulary=candidate_df.index.astype(str).unique(), mask_token=None\n",
    ")\n",
    "\n",
    "# Define the model\n",
    "class ArticleRetrievalModel(tfrs.models.Model):\n",
    "\n",
    "    def __init__(self,\n",
    "                 user_ids_vocabulary=user_ids_vocabulary,\n",
    "                 article_ids_vocabulary=article_ids_vocabulary,\n",
    "                 ):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "                \n",
    "        embedding_dim = 64  # dimensionality of learned user/article embeddings\n",
    "        \n",
    "        # User tower\n",
    "        self.user_model = Sequential([\n",
    "            user_ids_vocabulary,\n",
    "            layers.Embedding(\n",
    "                input_dim=user_ids_vocabulary.vocabulary_size(),\n",
    "                output_dim=embedding_dim)\n",
    "        ])\n",
    "        \n",
    "        # Article tower: combines id + precomputed embedding\n",
    "        self.article_id_model = Sequential([\n",
    "            article_ids_vocabulary,\n",
    "            layers.Embedding(article_ids_vocabulary.vocabulary_size(), \n",
    "                             embedding_dim)\n",
    "        ])\n",
    "        \n",
    "        self.article_embedding_projector = Sequential([\n",
    "            layers.Dense(128, activation=\"relu\"),\n",
    "            layers.Dense(embedding_dim)\n",
    "        ])\n",
    "        \n",
    "        # Retrieval task\n",
    "        self.task = tfrs.tasks.Retrieval()\n",
    "\n",
    "    def article_embedding(self, features):\n",
    "        # Combine ID-based embedding and precomputed 250D embedding\n",
    "        id_emb = self.article_id_model(features[\"article_id\"])\n",
    "        content_emb = self.article_embedding_projector(tf.convert_to_tensor(features[\"embedding\"]))\n",
    "        return tf.concat([id_emb, content_emb], axis=1)\n",
    "    \n",
    "    def compute_loss(self, features, training=False):\n",
    "        user_embeddings = self.user_model(features[\"user_id\"])\n",
    "        article_embeddings = self.article_embedding(features)\n",
    "        \n",
    "        return self.task(user_embeddings, article_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4b43670",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Text\n",
    "import tensorflow as tf\n",
    "\n",
    "class ArticldeModel(tfrs.Model):\n",
    "\n",
    "  def __init__(self, user_model, candidate_model, task):\n",
    "    super().__init__()\n",
    "    self.candidate_model: Model = candidate_model\n",
    "    self.user_model: Model = user_model\n",
    "    self.task: layers.Layer = task\n",
    "\n",
    "  def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:\n",
    "    # We pick out the user features and pass them into the user model.\n",
    "    user_embeddings = self.user_model(features[\"user_id\"])\n",
    "    # And pick out the movie features and pass them into the movie model,\n",
    "    # getting embeddings back.\n",
    "    positive_candidate_embeddings = self.candidate_model(features[\"article_id\"])\n",
    "\n",
    "    # The task computes the loss and the metrics.\n",
    "    return self.task(user_embeddings, positive_candidate_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b7c051",
   "metadata": {},
   "source": [
    "#### tfrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cdfa55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping\n",
    "from keras.optimizers import Adagrad\n",
    "from keras.losses import CategoricalCrossentropy\n",
    "\n",
    "def mlflow_experiment(rating_df = rating_df,\n",
    "                      candidate_df = candidate_df,\n",
    "                      ratings_keep= [\"user_id\",\"article_id\",\"time_per_word\"],\n",
    "                      candidates_keep = [\"article_id\"],                      \n",
    "                      rating_target = 'time_per_word',\n",
    "                      num_epochs = 5,\n",
    "                      **kwargs):\n",
    "    start_time = time.time()\n",
    "    # convert vocabulary to str\n",
    "    candidate_df = candidate_df.reset_index()\n",
    "\n",
    "    ratings_tf =  tf.data.Dataset.from_tensor_slices(dict(rating_df[ratings_keep]))\n",
    "    candidate_tf = tf.data.Dataset.from_tensor_slices(dict(candidate_df[candidates_keep]))\n",
    "\n",
    "    candidate_ids = candidate_df[\"article_id\"].astype(str)\n",
    "    user_ids = rating_df[\"user_id\"].astype(str)\n",
    "    # candidate_ids = candidate_tf.map(lambda x: x[\"article_id\"])\n",
    "    # user_ids = ratings_tf.map(lambda x: x[\"user_id\"])\n",
    "\n",
    "    \n",
    "    unique_candidate_id = candidate_ids.unique()\n",
    "    unique_user_ids = user_ids.unique()\n",
    "    # unique_user_ids = np.unique(np.concatenate(list(user_ids)))\n",
    "\n",
    "\n",
    "    # Build vocabularies\n",
    "    user_ids_vocabulary = layers.StringLookup(\n",
    "        vocabulary=unique_user_ids, mask_token=None\n",
    "    )\n",
    "    article_ids_vocabulary = layers.StringLookup(\n",
    "        vocabulary=unique_candidate_id, mask_token=None\n",
    "    )\n",
    "\n",
    "    #SPLIT test/train\n",
    "    tf.random.set_seed(42)\n",
    "    shuffled = ratings_tf.shuffle(len(rating_df), seed=42, reshuffle_each_iteration=False)\n",
    "    train = shuffled.take(int(len(rating_df) * 0.8))\n",
    "    test = shuffled.skip(int(len(rating_df) * 0.8))\n",
    "\n",
    "\n",
    "    # model = ArticleRetrievalModel(user_ids_vocabulary=user_ids_vocabulary,\n",
    "    #                               article_ids_vocabulary=article_ids_vocabulary)\n",
    "    #LOCAL MODEL\n",
    "    embedding_dimension = 32\n",
    "    user_model = Sequential([\n",
    "                 layers.StringLookup(\n",
    "                    vocabulary=unique_user_ids, mask_token=None),\n",
    "                # We add an additional embedding to account for unknown tokens.\n",
    "                 layers.Embedding(len(unique_user_ids) + 1, embedding_dimension)\n",
    "                ])\n",
    "    \n",
    "    candidate_model = Sequential([\n",
    "                  layers.StringLookup(\n",
    "                                    vocabulary=unique_candidate_id, mask_token=None),\n",
    "                  layers.Embedding(len(unique_candidate_id) + 1, embedding_dimension)\n",
    "    ])\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    # article_candidates = (\n",
    "    #     tf_df.batch(128)\n",
    "    #     #.map(candidate_model)\n",
    "    #     .map(model.article_id_model)\n",
    "    #     # .map(lambda x: (x[\"article_id\"], model.article_embedding(x)))\n",
    "    #     .cache()\n",
    "    # )\n",
    "    article_candidates = candidate_tf.batch(128).map(\n",
    "                                                    lambda x: candidate_model(x[\"article_id\"])\n",
    "                                                ).cache()\n",
    "    \n",
    "    candidate_embeddings = candidate_model(\n",
    "        tf.convert_to_tensor(candidate_df[\"article_id\"].astype(str))\n",
    "    )\n",
    "    candidate_dataset = tf.data.Dataset.from_tensor_slices({\n",
    "    \"article_id\": candidate_df[\"article_id\"].astype(str),\n",
    "    \"embedding\": candidate_embeddings  # Your pre-computed embeddings\n",
    "    })\n",
    "\n",
    "    # Assign metrics\n",
    "    # metrics=tfrs.metrics.FactorizedTopK(candidates=article_candidates)\n",
    "\n",
    "    # Explicitly specify k values and use the brute-force method\n",
    "    # metrics = tfrs.metrics.FactorizedTopK(\n",
    "    #     candidates=article_candidates,\n",
    "    #     ks=[1, 5, 10]  # Add this line\n",
    "    # )\n",
    "    # loss=CategoricalCrossentropy()\n",
    "\n",
    "    task = tfrs.tasks.Retrieval(\n",
    "        # metrics=metrics,\n",
    "        # loss = loss\n",
    "        \n",
    "    )\n",
    "\n",
    "    model = ArticldeModel(user_model,candidate_model,task)\n",
    "\n",
    "    # optimizer=Adagrad(learning_rate=0.1)\n",
    "    # optimizer=tf.compat.v1.train.AdagradOptimizer(learning_rate=0.1)\n",
    "    optimizer='adam'\n",
    "    model.compile(optimizer=optimizer,\n",
    "                #   loss=loss, \n",
    "                #   metrics=metrics\n",
    "                  )\n",
    "    \n",
    "    monitor=kwargs.get('monitor','val_loss')\n",
    "\n",
    "    callbacks = []\n",
    "    if kwargs.get('use_tensorboard',False):\n",
    "        tb = TensorBoard(log_dir='logs', write_graph=True)\n",
    "        callbacks.append(tb)\n",
    "    if kwargs.get('use_checkpoint',False):\n",
    "        mc = ModelCheckpoint(\n",
    "                            mode='max', \n",
    "                            filepath='models-dr/pdilated.weights.h5', \n",
    "                            monitor=monitor, \n",
    "                            save_best_only='True', \n",
    "                            save_weights_only='True', \n",
    "                            verbose=1)\n",
    "        callbacks.append(mc)\n",
    "    es = EarlyStopping(\n",
    "                        monitor=monitor,\n",
    "                        mode='min',\n",
    "                    #    mode='max', \n",
    "                    #    monitor='acc', \n",
    "                        patience= kwargs.get('patience',2), \n",
    "                        verbose=1)\n",
    "    callbacks.append(es)\n",
    "\n",
    "    cached_train = train.batch(8192).cache()\n",
    "    cached_test = test.batch(4096).cache()\n",
    "\n",
    "    # model.fit(cached_train, epochs=5)\n",
    "    with mlflow.start_run() as run:\n",
    "\n",
    "        # mlflow.tensorflow.autolog()\n",
    "        history =  model.fit( #fit_generator deprecated\n",
    "                    cached_train,\n",
    "                    # steps_per_epoch=train_step_per_epoch,\n",
    "                    epochs=num_epochs,\n",
    "                    # verbose=1,\n",
    "                    validation_data=cached_test,\n",
    "                    # validation_steps=val_step_per_epoch,\n",
    "                    # use_multiprocessing=True,\n",
    "                    # workers=16,\n",
    "                    # callbacks=callbacks,\n",
    "                    # max_queue_size=32,\n",
    "                    )\n",
    "\n",
    "        process_time = time.time() - start_time\n",
    "\n",
    "\n",
    "        # Start an MLflow run\n",
    "\n",
    "        signature = None\n",
    "        # Infer the model signature\n",
    "        # if sign_model:\n",
    "        #     signature = infer_signature(X_train, model.predict(X_train), model_params )\n",
    "\n",
    "\n",
    "        model_info  = mlflow.keras.log_model(\n",
    "                                    model=model,        \n",
    "                                    name=model.name,\n",
    "                                    signature=signature,\n",
    "                                    input_example=None,\n",
    "                                    registered_model_name=f\"{model.name}\",\n",
    "                                    )\n",
    "\n",
    "        \n",
    "        # hash_id = None\n",
    "        # try:\n",
    "        #     import hashlib\n",
    "        #     hash_id = hashlib.sha256(df.to_string().encode()).hexdigest()\n",
    "        # except:\n",
    "        #     pass\n",
    "\n",
    "        # Log other information about the model\n",
    "        mlflow.log_params({ \"Process_Time\": process_time,\n",
    "                        #    'ModelParams' : model_params,\n",
    "                            'optimizer':optimizer,\n",
    "                            # 'loss':loss,\n",
    "                            'monitor':monitor,\n",
    "                            # 'GenParams' : gen_params,\n",
    "                            'Metrics' : metrics,\n",
    "                            # 'TrainStepPerEpoch' : train_step_per_epoch, #sample/batch_size\n",
    "                            # 'ValStepPerEpoch' : val_step_per_epoch,\n",
    "                            'Epochs' : num_epochs,\n",
    "                            # 'DataHash': hash_id,\n",
    "                            # 'dataset_path':df_path,\n",
    "                            # 'dataset_length':len(df.index),\n",
    "                            \n",
    "                            })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf32cac2",
   "metadata": {},
   "source": [
    "#### tfrs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0a5ad9",
   "metadata": {},
   "source": [
    "https://github.com/tensorflow/recommenders/issues/712 Bug sur la librairie tfrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de0d6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mlflow_experiment(rating_df = rating_df,\n",
    "                candidate_df = candidate_df,\n",
    "                ratings_keep= [\"user_id\",\"article_id\",\"time_per_word\"],\n",
    "                candidates_keep = [\"article_id\"],                      \n",
    "                rating_target = 'time_per_word',\n",
    "                num_epochs = 5,\n",
    "                )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d78dd4",
   "metadata": {},
   "source": [
    "##### Get recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13892de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create article dataset with embeddings\n",
    "article_dataset = tf_df.batch(128).map(lambda x: (\n",
    "    x[\"article_id\"], model.article_embedding(x)\n",
    "))\n",
    "\n",
    "index = tfrs.layers.factorized_top_k.BruteForce(model.user_model)\n",
    "index.index_from_dataset(article_dataset)\n",
    "\n",
    "user_id = \"1\"\n",
    "scores, ids = index(tf.constant([user_id]))\n",
    "print(\"Top recommendations for user\", user_id, \":\", ids[0, :5].numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecd7f5b",
   "metadata": {},
   "source": [
    "## D. Recommenders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ddacaf",
   "metadata": {},
   "source": [
    "### LightFM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425e1c79",
   "metadata": {},
   "source": [
    "https://github.com/lyst/lightfm/issues/687 crash sans raison..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90509368",
   "metadata": {},
   "source": [
    "https://github.com/recommenders-team/recommenders/blob/main/examples/02_model_collaborative_filtering/lightfm_deep_dive.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99dd844a",
   "metadata": {},
   "source": [
    "incompatible python 3.13, 3.12.9, ? Installed from forked using new ctype for build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75b98a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System version: 3.12.9 | packaged by Anaconda, Inc. | (main, Feb  6 2025, 18:49:16) [MSC v.1929 64 bit (AMD64)]\n",
      "LightFM version: 1.17\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import lightfm\n",
    "from lightfm import LightFM\n",
    "from lightfm.data import Dataset\n",
    "from lightfm import cross_validation\n",
    "from lightfm.evaluation import precision_at_k as lightfm_prec_at_k\n",
    "from lightfm.evaluation import recall_at_k as lightfm_recall_at_k\n",
    "\n",
    "from recommenders.evaluation.python_evaluation import precision_at_k, recall_at_k\n",
    "from recommenders.utils.timer import Timer\n",
    "from recommenders.datasets import movielens\n",
    "from recommenders.models.lightfm.lightfm_utils import (\n",
    "    track_model_metrics,\n",
    "    prepare_test_df,\n",
    "    prepare_all_predictions,\n",
    "    compare_metric,\n",
    "    similar_users,\n",
    "    similar_items,\n",
    ")\n",
    "from recommenders.utils.notebook_utils import store_metadata\n",
    "\n",
    "print(\"System version: {}\".format(sys.version))\n",
    "print(\"LightFM version: {}\".format(lightfm.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f342563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default number of recommendations\n",
    "K = 10\n",
    "# percentage of data used for testing\n",
    "TEST_PERCENTAGE = 0.25\n",
    "# model learning rate\n",
    "LEARNING_RATE = 0.25\n",
    "# no of latent factors\n",
    "NO_COMPONENTS = 20\n",
    "# no of epochs to fit model\n",
    "NO_EPOCHS = 20\n",
    "# no of threads to fit model\n",
    "NO_THREADS = 4\n",
    "# regularisation for both user and item features\n",
    "ITEM_ALPHA = 1e-6\n",
    "USER_ALPHA = 1e-6\n",
    "\n",
    "# seed for pseudonumber generations\n",
    "SEED = 42\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26aabcb6",
   "metadata": {},
   "source": [
    "#### Prepared dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06a68563",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset()\n",
    "\n",
    "dataset.fit(users=rating_df['user_id'], \n",
    "            items=rating_df['article_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fcd2b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num users: 322897, num_topics: 46033.\n"
     ]
    }
   ],
   "source": [
    "ratings_target = ['time_per_word']\n",
    "\n",
    "\n",
    "num_users, num_articles = dataset.interactions_shape()\n",
    "print(f'Num users: {num_users}, num_topics: {num_articles}.')\n",
    "\n",
    "(interactions, weights) = dataset.build_interactions(rating_df[['user_id','article_id']+ratings_target].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ed9661",
   "metadata": {},
   "source": [
    "#### Train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6987c3de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train interactions: (322897, 46033)\n",
      "Shape of test interactions: (322897, 46033)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_interactions, test_interactions = cross_validation.random_train_test_split(\n",
    "    interactions, test_percentage=TEST_PERCENTAGE,\n",
    "    random_state=np.random.RandomState(SEED))\n",
    "\n",
    "print(f\"Shape of train interactions: {train_interactions.shape}\")\n",
    "print(f\"Shape of test interactions: {test_interactions.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d82a34e",
   "metadata": {},
   "source": [
    "#### Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c07cf50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model1 = LightFM(loss='warp', no_components=NO_COMPONENTS, \n",
    "                 learning_rate=LEARNING_RATE,                 \n",
    "                 random_state=np.random.RandomState(SEED))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f220162",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model1.fit(interactions=train_interactions,\n",
    "          epochs=NO_EPOCHS)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47579469",
   "metadata": {},
   "source": [
    "### DKN? https://github.com/recommenders-team/recommenders/blob/main/examples/00_quick_start/dkn_MIND.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d3533c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projet3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
