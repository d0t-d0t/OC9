{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3e28e24",
   "metadata": {},
   "source": [
    "## Import initiaux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34c89d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "import pickle\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "ARTICLES_PATH = \"Datas/articles_metadata.csv\"  \n",
    "CLICKS_SAMPLE_PATH = \"Datas/clicks_sample.csv\"     \n",
    "CLICKS__PATH = \"Datas/clicks/\"         \n",
    "EMBEDING_PATH = \"Datas/articles_embeddings.pickle\" \n",
    "RATING_PREPROC_DF_PATH='Datas/rating_preprocess_df.pkl'\n",
    "CANDIDATE_PREPROC_DF_PATH='Datas/candidate_preprocess_df.pkl'\n",
    "\n",
    "debug=False\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7857e6",
   "metadata": {},
   "source": [
    "# I. Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc021aa",
   "metadata": {},
   "source": [
    "\n",
    "## A. Articles_metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7d502e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Articles shape: (364047, 5)\n",
      "\n",
      "ARTICLES METADATA:\n",
      "Number of articles: 364047\n",
      "Number of columns: 5\n",
      "\n",
      "Columns:\n",
      "  - article_id\n",
      "  - category_id\n",
      "  - created_at_ts\n",
      "  - publisher_id\n",
      "  - words_count\n",
      "\n",
      "articles data head:\n",
      "   article_id  category_id  created_at_ts  publisher_id  words_count\n",
      "0           0            0  1513144419000             0          168\n",
      "1           1            1  1405341936000             0          189\n",
      "2           2            1  1408667706000             0          250\n",
      "3           3            1  1408468313000             0          230\n",
      "4           4            1  1407071171000             0          162\n",
      "Articles dataset:\n",
      "article_id       int64\n",
      "category_id      int64\n",
      "created_at_ts    int64\n",
      "publisher_id     int64\n",
      "words_count      int64\n",
      "dtype: object\n",
      "Missing Values:\n",
      "article_id       0\n",
      "category_id      0\n",
      "created_at_ts    0\n",
      "publisher_id     0\n",
      "words_count      0\n",
      "dtype: int64\n",
      "Duplicate articles: 0\n"
     ]
    }
   ],
   "source": [
    "# Load articles metadata\n",
    "articles_df = pd.read_csv(ARTICLES_PATH)\n",
    "print(f\"Articles shape: {articles_df.shape}\")\n",
    "\n",
    "# Articles dataset info\n",
    "print(\"\\nARTICLES METADATA:\")\n",
    "print(f\"Number of articles: {len(articles_df)}\")\n",
    "print(f\"Number of columns: {len(articles_df.columns)}\")\n",
    "print(\"\\nColumns:\")\n",
    "for col in articles_df.columns:\n",
    "    print(f\"  - {col}\")\n",
    "\n",
    "print(\"\\narticles data head:\")\n",
    "print(articles_df.head())\n",
    "\n",
    "print(\"Articles dataset:\")\n",
    "print(articles_df.dtypes)\n",
    "\n",
    "print(\"Missing Values:\")\n",
    "print(articles_df.isnull().sum())\n",
    "\n",
    "print(f\"Duplicate articles: {articles_df.duplicated().sum()}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ef13d3",
   "metadata": {},
   "source": [
    "la pluspart des informations vont nous êtres inutiles:\n",
    "    - l'id servira au merge avec les autres df \n",
    "    - on peut conserver  la catégorie même si elle sera probablement redondante avec l'embediing\n",
    "    - words_count pourrait servir à pondérer le temps de rétentention, si on peut le calculer avec la longueur de l'article"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285c72af",
   "metadata": {},
   "source": [
    "## B. Clicks_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83983dd",
   "metadata": {},
   "source": [
    "### Load datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4267e469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: Datas/clicks/clicks_hour_000.csv\n",
      "Loading: Datas/clicks/clicks_hour_001.csv\n",
      "Loading: Datas/clicks/clicks_hour_002.csv\n",
      "Loading: Datas/clicks/clicks_hour_003.csv\n",
      "Loading: Datas/clicks/clicks_hour_004.csv\n",
      "Loading: Datas/clicks/clicks_hour_005.csv\n",
      "Loading: Datas/clicks/clicks_hour_006.csv\n",
      "Loading: Datas/clicks/clicks_hour_007.csv\n",
      "Loading: Datas/clicks/clicks_hour_008.csv\n",
      "Loading: Datas/clicks/clicks_hour_009.csv\n",
      "Loading: Datas/clicks/clicks_hour_010.csv\n",
      "Loading: Datas/clicks/clicks_hour_011.csv\n",
      "Loading: Datas/clicks/clicks_hour_012.csv\n",
      "Loading: Datas/clicks/clicks_hour_013.csv\n",
      "Loading: Datas/clicks/clicks_hour_014.csv\n",
      "Loading: Datas/clicks/clicks_hour_015.csv\n",
      "Loading: Datas/clicks/clicks_hour_016.csv\n",
      "Loading: Datas/clicks/clicks_hour_017.csv\n",
      "Loading: Datas/clicks/clicks_hour_018.csv\n",
      "Loading: Datas/clicks/clicks_hour_019.csv\n",
      "Loading: Datas/clicks/clicks_hour_020.csv\n",
      "Loading: Datas/clicks/clicks_hour_021.csv\n",
      "Loading: Datas/clicks/clicks_hour_022.csv\n",
      "Loading: Datas/clicks/clicks_hour_023.csv\n",
      "Loading: Datas/clicks/clicks_hour_024.csv\n",
      "Loading: Datas/clicks/clicks_hour_025.csv\n",
      "Loading: Datas/clicks/clicks_hour_026.csv\n",
      "Loading: Datas/clicks/clicks_hour_027.csv\n",
      "Loading: Datas/clicks/clicks_hour_028.csv\n",
      "Loading: Datas/clicks/clicks_hour_029.csv\n",
      "Loading: Datas/clicks/clicks_hour_030.csv\n",
      "Loading: Datas/clicks/clicks_hour_031.csv\n",
      "Loading: Datas/clicks/clicks_hour_032.csv\n",
      "Loading: Datas/clicks/clicks_hour_033.csv\n",
      "Loading: Datas/clicks/clicks_hour_034.csv\n",
      "Loading: Datas/clicks/clicks_hour_035.csv\n",
      "Loading: Datas/clicks/clicks_hour_036.csv\n",
      "Loading: Datas/clicks/clicks_hour_037.csv\n",
      "Loading: Datas/clicks/clicks_hour_038.csv\n",
      "Loading: Datas/clicks/clicks_hour_039.csv\n",
      "Loading: Datas/clicks/clicks_hour_040.csv\n",
      "Loading: Datas/clicks/clicks_hour_041.csv\n",
      "Loading: Datas/clicks/clicks_hour_042.csv\n",
      "Loading: Datas/clicks/clicks_hour_043.csv\n",
      "Loading: Datas/clicks/clicks_hour_044.csv\n",
      "Loading: Datas/clicks/clicks_hour_045.csv\n",
      "Loading: Datas/clicks/clicks_hour_046.csv\n",
      "Loading: Datas/clicks/clicks_hour_047.csv\n",
      "Loading: Datas/clicks/clicks_hour_048.csv\n",
      "Loading: Datas/clicks/clicks_hour_049.csv\n",
      "Loading: Datas/clicks/clicks_hour_050.csv\n",
      "Loading: Datas/clicks/clicks_hour_051.csv\n",
      "Loading: Datas/clicks/clicks_hour_052.csv\n",
      "Loading: Datas/clicks/clicks_hour_053.csv\n",
      "Loading: Datas/clicks/clicks_hour_054.csv\n",
      "Loading: Datas/clicks/clicks_hour_055.csv\n",
      "Loading: Datas/clicks/clicks_hour_056.csv\n",
      "Loading: Datas/clicks/clicks_hour_057.csv\n",
      "Loading: Datas/clicks/clicks_hour_058.csv\n",
      "Loading: Datas/clicks/clicks_hour_059.csv\n",
      "Loading: Datas/clicks/clicks_hour_060.csv\n",
      "Loading: Datas/clicks/clicks_hour_061.csv\n",
      "Loading: Datas/clicks/clicks_hour_062.csv\n",
      "Loading: Datas/clicks/clicks_hour_063.csv\n",
      "Loading: Datas/clicks/clicks_hour_064.csv\n",
      "Loading: Datas/clicks/clicks_hour_065.csv\n",
      "Loading: Datas/clicks/clicks_hour_066.csv\n",
      "Loading: Datas/clicks/clicks_hour_067.csv\n",
      "Loading: Datas/clicks/clicks_hour_068.csv\n",
      "Loading: Datas/clicks/clicks_hour_069.csv\n",
      "Loading: Datas/clicks/clicks_hour_070.csv\n",
      "Loading: Datas/clicks/clicks_hour_071.csv\n",
      "Loading: Datas/clicks/clicks_hour_072.csv\n",
      "Loading: Datas/clicks/clicks_hour_073.csv\n",
      "Loading: Datas/clicks/clicks_hour_074.csv\n",
      "Loading: Datas/clicks/clicks_hour_075.csv\n",
      "Loading: Datas/clicks/clicks_hour_076.csv\n",
      "Loading: Datas/clicks/clicks_hour_077.csv\n",
      "Loading: Datas/clicks/clicks_hour_078.csv\n",
      "Loading: Datas/clicks/clicks_hour_079.csv\n",
      "Loading: Datas/clicks/clicks_hour_080.csv\n",
      "Loading: Datas/clicks/clicks_hour_081.csv\n",
      "Loading: Datas/clicks/clicks_hour_082.csv\n",
      "Loading: Datas/clicks/clicks_hour_083.csv\n",
      "Loading: Datas/clicks/clicks_hour_084.csv\n",
      "Loading: Datas/clicks/clicks_hour_085.csv\n",
      "Loading: Datas/clicks/clicks_hour_086.csv\n",
      "Loading: Datas/clicks/clicks_hour_087.csv\n",
      "Loading: Datas/clicks/clicks_hour_088.csv\n",
      "Loading: Datas/clicks/clicks_hour_089.csv\n",
      "Loading: Datas/clicks/clicks_hour_090.csv\n",
      "Loading: Datas/clicks/clicks_hour_091.csv\n",
      "Loading: Datas/clicks/clicks_hour_092.csv\n",
      "Loading: Datas/clicks/clicks_hour_093.csv\n",
      "Loading: Datas/clicks/clicks_hour_094.csv\n",
      "Loading: Datas/clicks/clicks_hour_095.csv\n",
      "Loading: Datas/clicks/clicks_hour_096.csv\n",
      "Loading: Datas/clicks/clicks_hour_097.csv\n",
      "Loading: Datas/clicks/clicks_hour_098.csv\n",
      "Loading: Datas/clicks/clicks_hour_099.csv\n",
      "Loading: Datas/clicks/clicks_hour_100.csv\n",
      "Loading: Datas/clicks/clicks_hour_101.csv\n",
      "Loading: Datas/clicks/clicks_hour_102.csv\n",
      "Loading: Datas/clicks/clicks_hour_103.csv\n",
      "Loading: Datas/clicks/clicks_hour_104.csv\n",
      "Loading: Datas/clicks/clicks_hour_105.csv\n",
      "Loading: Datas/clicks/clicks_hour_106.csv\n",
      "Loading: Datas/clicks/clicks_hour_107.csv\n",
      "Loading: Datas/clicks/clicks_hour_108.csv\n",
      "Loading: Datas/clicks/clicks_hour_109.csv\n",
      "Loading: Datas/clicks/clicks_hour_110.csv\n",
      "Loading: Datas/clicks/clicks_hour_111.csv\n",
      "Loading: Datas/clicks/clicks_hour_112.csv\n",
      "Loading: Datas/clicks/clicks_hour_113.csv\n",
      "Loading: Datas/clicks/clicks_hour_114.csv\n",
      "Loading: Datas/clicks/clicks_hour_115.csv\n",
      "Loading: Datas/clicks/clicks_hour_116.csv\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     12\u001b[39m         file_path = os.path.join(CLICKS__PATH, file)\n\u001b[32m     13\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoading: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m         clicks_files.append(\u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     17\u001b[39m clicks_df = pd.concat(clicks_files, ignore_index=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCombined \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(clicks_files)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m click files\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\d0t\\anaconda3\\envs\\projet3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\d0t\\anaconda3\\envs\\projet3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\d0t\\anaconda3\\envs\\projet3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\d0t\\anaconda3\\envs\\projet3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\d0t\\anaconda3\\envs\\projet3\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen codecs>:309\u001b[39m, in \u001b[36m__init__\u001b[39m\u001b[34m(self, errors)\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "if debug:\n",
    "    clicks_df = pd.read_csv(CLICKS_SAMPLE_PATH)\n",
    "else:\n",
    "    clicks_files = []\n",
    "        \n",
    "    # Check if CLICKS_PATH is a directory or file\n",
    "    if os.path.isdir(CLICKS__PATH):\n",
    "        # Load all CSV files in the directory\n",
    "        for file in os.listdir(CLICKS__PATH):\n",
    "            if file.endswith('.csv'):\n",
    "                file_path = os.path.join(CLICKS__PATH, file)\n",
    "                print(f\"Loading: {file_path}\")\n",
    "                clicks_files.append(pd.read_csv(file_path))\n",
    "        \n",
    "\n",
    "        clicks_df = pd.concat(clicks_files, ignore_index=True)\n",
    "        print(f\"Combined {len(clicks_files)} click files\")\n",
    "                                   \n",
    "#rename click_article_df for merge\n",
    "clicks_df.rename(columns={'click_article_id':'article_id'}, errors=\"raise\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7fcc0805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clicks shape: (2988181, 12)\n",
      "Number of interactions: 2988181\n",
      "Number of unique users: 322897\n",
      "Date range: 1506826800026 to 1510603454886\n",
      "\n",
      "First few rows of clicks data:\n",
      "  user_id        session_id  session_start session_size article_id  \\\n",
      "0       0  1506825423271737  1506825423000            2     157541   \n",
      "1       0  1506825423271737  1506825423000            2      68866   \n",
      "2       1  1506825426267738  1506825426000            2     235840   \n",
      "3       1  1506825426267738  1506825426000            2      96663   \n",
      "4       2  1506825435299739  1506825435000            2     119592   \n",
      "\n",
      "  click_timestamp click_environment click_deviceGroup click_os click_country  \\\n",
      "0   1506826828020                 4                 3       20             1   \n",
      "1   1506826858020                 4                 3       20             1   \n",
      "2   1506827017951                 4                 1       17             1   \n",
      "3   1506827047951                 4                 1       17             1   \n",
      "4   1506827090575                 4                 1       17             1   \n",
      "\n",
      "  click_region click_referrer_type  \n",
      "0           20                   2  \n",
      "1           20                   2  \n",
      "2           16                   2  \n",
      "3           16                   2  \n",
      "4           24                   2  \n",
      "\n",
      " MISSING VALUES:\n",
      "user_id                0\n",
      "session_id             0\n",
      "session_start          0\n",
      "session_size           0\n",
      "article_id             0\n",
      "click_timestamp        0\n",
      "click_environment      0\n",
      "click_deviceGroup      0\n",
      "click_os               0\n",
      "click_country          0\n",
      "click_region           0\n",
      "click_referrer_type    0\n",
      "dtype: int64\n",
      "\n",
      " DUPLICATES:\n",
      "0\n",
      "\n",
      " DATA TYPES:\n",
      "user_id                object\n",
      "session_id             object\n",
      "session_start          object\n",
      "session_size           object\n",
      "article_id             object\n",
      "click_timestamp        object\n",
      "click_environment      object\n",
      "click_deviceGroup      object\n",
      "click_os               object\n",
      "click_country          object\n",
      "click_region           object\n",
      "click_referrer_type    object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(f\"Clicks shape: {clicks_df.shape}\")\n",
    "\n",
    "print(f\"Number of interactions: {len(clicks_df)}\")\n",
    "print(f\"Number of unique users: {clicks_df['user_id'].nunique()}\")\n",
    "print(f\"Date range: {clicks_df['click_timestamp'].min()} to {clicks_df['click_timestamp'].max()}\")\n",
    "\n",
    "print(\"\\nFirst few rows of clicks data:\")\n",
    "print(clicks_df.head())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\n MISSING VALUES:\")\n",
    "print(clicks_df.isnull().sum())\n",
    "\n",
    "# Check for duplicates\n",
    "print(f\"\\n DUPLICATES:\")\n",
    "print(clicks_df.duplicated().sum())\n",
    "\n",
    "# Check data types\n",
    "print(\"\\n DATA TYPES:\")\n",
    "print(clicks_df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16384c67",
   "metadata": {},
   "source": [
    "clicks_{}.csv contient 12 collonnes :\n",
    "\n",
    "    user_id : user ID\n",
    "    session_id : Session ID\n",
    "    session_start : Début de session (timestamp)\n",
    "    session_size : nombre d'article vu sur la session\n",
    "    click_article_id : article ID user clicked\n",
    "    click_timestamp : When user clicked (timestamp)\n",
    "    click_environment : user env\n",
    "    click_deviceGroup : user device\n",
    "    click_os : user OS\n",
    "    click_country : localisation (country)\n",
    "    click_region : localisation (region)\n",
    "    click_referrer_type : ?\n",
    "\n",
    "On peut drop \n",
    "    click_environment      \n",
    "    click_deviceGroup      \n",
    "    click_os               \n",
    "    click_country          \n",
    "    click_region           \n",
    "    click_referrer_type    \n",
    "\n",
    "On peut merge les metadatas conservées\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1d8067",
   "metadata": {},
   "source": [
    "On récupère la différence entre le précédant click et le clic actuel pour avoir le temps assé sur chaque article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f724b6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "clicks_df = clicks_df.apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ff67479a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "time_spend_on_article",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "79263127-86b7-4568-9a3b-e383339636dc",
       "rows": [
        [
         "count",
         "2988181.0"
        ],
        [
         "mean",
         "569876.0059089459"
        ],
        [
         "std",
         "5442533.483914518"
        ],
        [
         "min",
         "0.0"
        ],
        [
         "25%",
         "30000.0"
        ],
        [
         "50%",
         "39903.0"
        ],
        [
         "75%",
         "202575.0"
        ],
        [
         "max",
         "1212149256.0"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 8
       }
      },
      "text/plain": [
       "count    2.988181e+06\n",
       "mean     5.698760e+05\n",
       "std      5.442533e+06\n",
       "min      0.000000e+00\n",
       "25%      3.000000e+04\n",
       "50%      3.990300e+04\n",
       "75%      2.025750e+05\n",
       "max      1.212149e+09\n",
       "Name: time_spend_on_article, dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clicks_df['prev_click_timestamp'] = clicks_df.groupby('session_id')['click_timestamp'].shift(1)\n",
    "clicks_df.loc[clicks_df['prev_click_timestamp'].isna(),'prev_click_timestamp'] = clicks_df['session_start']\n",
    "clicks_df['time_spend_on_article'] = clicks_df['click_timestamp']-clicks_df['prev_click_timestamp']\n",
    "\n",
    "clicks_df['time_spend_on_article'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efe37a2",
   "metadata": {},
   "source": [
    "On comprend qu'un timestamp automatique à +30000 a été appliqué à toutes les dernières entrées de session, pour lesquelles on a pas de timestamp de sortie.\n",
    "\n",
    "On peut soit essayer de corriger ça en remplaçant par exemple par la durée moyenne de lecture pour une meilleur approximation, soit abandonner la pondération par temps de lecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "91180752",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_timespend = clicks_df.loc[clicks_df['time_spend_on_article']!=30000,'time_spend_on_article'].mean()\n",
    "clicks_df.loc[clicks_df['time_spend_on_article']==30000,'time_spend_on_article']=mean_timespend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "72ba97f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "clicks_df.drop(columns = ['session_id',\n",
    "                'session_size',\n",
    "                'click_environment',\n",
    "                'click_deviceGroup',\n",
    "                'click_os',\n",
    "                'click_country',\n",
    "                'click_region',\n",
    "                'click_referrer_type',\n",
    "                'prev_click_timestamp'],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7560f61a",
   "metadata": {},
   "source": [
    "On va avoir besoind e récuperer certaines metadata des articles pour créer notre rating. On merge donc les DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4e7e6b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged shape: (2988181, 9)\n"
     ]
    }
   ],
   "source": [
    "# Merge datasets\n",
    "clicks_df = clicks_df.merge(\n",
    "    articles_df, \n",
    "    on='article_id', \n",
    "    how='left'\n",
    ")\n",
    "print(f\"Merged shape: {clicks_df.shape}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01f475c",
   "metadata": {},
   "source": [
    "#### Rating\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "02b7a4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "clicks_df['time_per_word'] = clicks_df['time_spend_on_article']/clicks_df['words_count']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829f1965",
   "metadata": {},
   "source": [
    "## C. articles_embeding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e7c894",
   "metadata": {},
   "source": [
    "- Le pickle article_embedding est une représentation abstraites des articles qui vas nous servir pour la recommendation:\n",
    "    Pickle (Python 3) of a NumPy matrix containing the Article Content Embeddings (250-dimensional vectors), trained upon articles' text and metadata by the CHAMELEON's ACR module (see paper for details) for 364047 published articles.\n",
    "    P.s. The full text of news articles could not be provided due to license restrictions, but those embeddings can be used by Neural Networks to represent their content. See this paper for a t-SNE visualization of these embeddings, colored by category.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e49fef28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeding shape: (364047, 250)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open(EMBEDING_PATH , 'rb') as f:    \n",
    "    embeding_df = pickle.load(f)\n",
    "\n",
    "embeding_df = pd.DataFrame(embeding_df)\n",
    "\n",
    "\n",
    "print(f\"Embeding shape: {embeding_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4047218c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings_with_id = embeding_df.reset_index().rename(columns={'index': 'article_id'})\n",
    "embeding_df.index.names = ['article_id']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657fcce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Merge datasets\n",
    "# merged_df = merged_df.merge(\n",
    "#     embeding_df, \n",
    "#     on='article_id', \n",
    "#     how='left'\n",
    "# )\n",
    "# print(f\"Merged shape: {merged_df.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e866a9c",
   "metadata": {},
   "source": [
    " la data ne semble pas fiable pour determiner le temps passé sur els article:\n",
    "    Sur les session de 2 article la difference de click_timestamp est toujours identique.\n",
    "    il n'existe aucune session de 1 article\n",
    "    l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ef972c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clicks_df.to_pickle(RATING_PREPROC_DF_PATH)\n",
    "embeding_df.to_pickle(CANDIDATE_PREPROC_DF_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d435bf7",
   "metadata": {},
   "source": [
    "# II. Model de recommandation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81a4075f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(RATING_PREPROC_DF_PATH , 'rb') as f:\n",
    "    rating_df = pickle.load(f)\n",
    "with open(CANDIDATE_PREPROC_DF_PATH , 'rb') as f:\n",
    "    candidate_df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9638556",
   "metadata": {},
   "source": [
    "##  librairie: implicit vs surprise vs tensorflow reocmmendation:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea2fbff",
   "metadata": {},
   "source": [
    "Implicit n'est pas en V1 et n'a pas vu de commit depuis plus d'un an. Il n'y a pas de documentation hormis  le git.\n",
    "\n",
    "Surprise est mieux maintenu  et plus cité mais plus orienté explicit qu'implicit\n",
    "\n",
    "tfrs est ~~bien maintenu~~(non), addossé a tensorflow, et adapté au problème:\n",
    "https://www.tensorflow.org/recommenders/examples/basic_retrieval\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67f7bbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pprint\n",
    "candidate_tfds = tf.data.Dataset.from_tensor_slices(candidate_df)\n",
    "rating_tfds = tf.data.Dataset.from_tensor_slices(rating_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b22324c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([0.00000000e+00, 1.50682542e+12, 1.57541000e+05, 1.50682683e+12,\n",
      "       1.40502000e+06, 2.81000000e+02, 1.50680052e+12, 0.00000000e+00,\n",
      "       2.80000000e+02, 5.01792857e+03])\n"
     ]
    }
   ],
   "source": [
    "for x in rating_tfds.take(1).as_numpy_iterator():\n",
    "  pprint.pprint(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a03038",
   "metadata": {},
   "source": [
    "## A. IMPLICIT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc02a1d7",
   "metadata": {},
   "source": [
    "### 1. Model architecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3f40330",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\d0t\\anaconda3\\envs\\projet3\\Lib\\site-packages\\mlflow\\pyfunc\\utils\\data_validation.py:186: UserWarning: \u001b[33mAdd type hints to the `predict` method to enable data validation and automatic signature inference during model logging. Check https://mlflow.org/docs/latest/model/python_model.html#type-hint-usage-in-pythonmodel for more details.\u001b[0m\n",
      "  color_warning(\n"
     ]
    }
   ],
   "source": [
    "from implicit_model import ArticleRetrievalImplicit\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70ba1165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix shape: (322897, 46033)\n",
      "Training ALS model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:06<00:00,  7.71it/s, train_auc=98.30%, skipped=4.99%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After fit:\n",
      "user_item shape: (322897, 46033)\n",
      "Training complete.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "retrieval_model = ArticleRetrievalImplicit(rating_df, \n",
    "                                           candidate_df, \n",
    "                                           factors=64,\n",
    "                                           model_type='BAY'\n",
    "                                           )\n",
    "retrieval_model.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dea37b1",
   "metadata": {},
   "source": [
    "### 2. Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e8524e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from implicit_model import implicit_evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b13dd5",
   "metadata": {},
   "source": [
    "#### test de recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c7b01847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'article_id': '331242', 'score': 7.332470417022705}, {'article_id': '331664', 'score': 6.8674774169921875}, {'article_id': '10253', 'score': 6.760776042938232}, {'article_id': '353786', 'score': 6.688801288604736}, {'article_id': '36160', 'score': 6.6756181716918945}]\n"
     ]
    }
   ],
   "source": [
    "# Recommend articles for a user\n",
    "result = retrieval_model.recommend(\"8\", N=5)\n",
    "print(result)\n",
    "\n",
    "# Find similar articles\n",
    "# print(retrieval_model.similar_items('42', N=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b9cfd5",
   "metadata": {},
   "source": [
    "### 3. Entrainement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361c72c8",
   "metadata": {},
   "source": [
    "#### Initialisation mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f81e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow_tools import start_local_experiment\n",
    "start_local_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc733a70",
   "metadata": {},
   "source": [
    "#### Experimentation mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c9432c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlflow_experiment(rating_df = rating_df,\n",
    "                      candidate_df = candidate_df,\n",
    "                      ratings_keep= [\"user_id\",\"article_id\",\"time_per_word\"],\n",
    "                      candidates_keep = [\"article_id\"],                      \n",
    "                      rating_target = 'time_per_word',\n",
    "                      train_test_split_perc = 0.8,\n",
    "                      num_epochs = 5,\n",
    "                      **kwargs):\n",
    "    start_time = time.time()\n",
    "    # convert vocabulary to str\n",
    "    candidate_df = candidate_df.reset_index()\n",
    "\n",
    "    rating_df = rating_df[ratings_keep]\n",
    "    candidate_df = candidate_df[candidates_keep]\n",
    "\n",
    "\n",
    "    model = ArticleRetrievalImplicit(rating_df=rating_df,\n",
    "                                     candidate_df=candidate_df,\n",
    "                                     rating_target=rating_target,\n",
    "                                     train_test_split_perc = train_test_split_perc,\n",
    "                                     embeding_alpha= kwargs.get('embeding_alpha',0.8),\n",
    "                                     factors=kwargs.get('factors',64),\n",
    "                                     add_embeding_vector=kwargs.get('add_embeding_vector',False)\n",
    "                                     )  \n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    \n",
    "    evaluation_dic = implicit_evaluation(model)\n",
    "\n",
    "    process_time = time.time() - start_time\n",
    "\n",
    "\n",
    "    with mlflow.start_run() as run:\n",
    "\n",
    "        for k,v in evaluation_dic.items():\n",
    "            mlflow.log_metric(k, v)\n",
    "\n",
    "\n",
    "        signature = None\n",
    "        params_dic = {}\n",
    "        params_dic[\"Process_Time\"] = process_time\n",
    "        params_dic[\"train_test_split_perc\"] = train_test_split_perc\n",
    "        params_dic[\"ratings_keep\"] = ratings_keep\n",
    "        params_dic[\"candidates_keep\"] = candidates_keep\n",
    "        params_dic[\"rating_target\"] = rating_target\n",
    "        \n",
    "        for k,v in kwargs.items():\n",
    "            params_dic[k] = v\n",
    "\n",
    "        \n",
    "        model_info  = mlflow.pyfunc.log_model(\n",
    "                                    python_model=model,        \n",
    "                                    name=model.name,\n",
    "                                    signature=signature,\n",
    "                                    input_example=None,\n",
    "                                    # registered_model_name=f\"{model.name}\",\n",
    "                                    )\n",
    "\n",
    "        # Log other information about the model\n",
    "        mlflow.log_params(params_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14d484a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mlflow_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db8ca1a",
   "metadata": {},
   "source": [
    "## B. Keras+tfrs (abandonné car mal maintenu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b362b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Sequential,layers,Model\n",
    "\n",
    "import tensorflow_recommenders as tfrs\n",
    "\n",
    "\n",
    "# Build vocabularies\n",
    "user_ids_vocabulary = layers.StringLookup(\n",
    "    vocabulary=rating_df[\"user_id\"].astype(str).unique(), mask_token=None\n",
    ")\n",
    "article_ids_vocabulary = layers.StringLookup(\n",
    "    vocabulary=candidate_df.index.astype(str).unique(), mask_token=None\n",
    ")\n",
    "\n",
    "# Define the model\n",
    "class ArticleRetrievalModel(tfrs.models.Model):\n",
    "\n",
    "    def __init__(self,\n",
    "                 user_ids_vocabulary=user_ids_vocabulary,\n",
    "                 article_ids_vocabulary=article_ids_vocabulary,\n",
    "                 ):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "                \n",
    "        embedding_dim = 64  # dimensionality of learned user/article embeddings\n",
    "        \n",
    "        # User tower\n",
    "        self.user_model = Sequential([\n",
    "            user_ids_vocabulary,\n",
    "            layers.Embedding(\n",
    "                input_dim=user_ids_vocabulary.vocabulary_size(),\n",
    "                output_dim=embedding_dim)\n",
    "        ])\n",
    "        \n",
    "        # Article tower: combines id + precomputed embedding\n",
    "        self.article_id_model = Sequential([\n",
    "            article_ids_vocabulary,\n",
    "            layers.Embedding(article_ids_vocabulary.vocabulary_size(), \n",
    "                             embedding_dim)\n",
    "        ])\n",
    "        \n",
    "        self.article_embedding_projector = Sequential([\n",
    "            layers.Dense(128, activation=\"relu\"),\n",
    "            layers.Dense(embedding_dim)\n",
    "        ])\n",
    "        \n",
    "        # Retrieval task\n",
    "        self.task = tfrs.tasks.Retrieval()\n",
    "\n",
    "    def article_embedding(self, features):\n",
    "        # Combine ID-based embedding and precomputed 250D embedding\n",
    "        id_emb = self.article_id_model(features[\"article_id\"])\n",
    "        content_emb = self.article_embedding_projector(tf.convert_to_tensor(features[\"embedding\"]))\n",
    "        return tf.concat([id_emb, content_emb], axis=1)\n",
    "    \n",
    "    def compute_loss(self, features, training=False):\n",
    "        user_embeddings = self.user_model(features[\"user_id\"])\n",
    "        article_embeddings = self.article_embedding(features)\n",
    "        \n",
    "        return self.task(user_embeddings, article_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4b43670",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Text\n",
    "import tensorflow as tf\n",
    "\n",
    "class ArticldeModel(tfrs.Model):\n",
    "\n",
    "  def __init__(self, user_model, candidate_model, task):\n",
    "    super().__init__()\n",
    "    self.candidate_model: Model = candidate_model\n",
    "    self.user_model: Model = user_model\n",
    "    self.task: layers.Layer = task\n",
    "\n",
    "  def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:\n",
    "    # We pick out the user features and pass them into the user model.\n",
    "    user_embeddings = self.user_model(features[\"user_id\"])\n",
    "    # And pick out the movie features and pass them into the movie model,\n",
    "    # getting embeddings back.\n",
    "    positive_candidate_embeddings = self.candidate_model(features[\"article_id\"])\n",
    "\n",
    "    # The task computes the loss and the metrics.\n",
    "    return self.task(user_embeddings, positive_candidate_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b7c051",
   "metadata": {},
   "source": [
    "#### tfrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cdfa55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping\n",
    "from keras.optimizers import Adagrad\n",
    "from keras.losses import CategoricalCrossentropy\n",
    "\n",
    "def mlflow_experiment(rating_df = rating_df,\n",
    "                      candidate_df = candidate_df,\n",
    "                      ratings_keep= [\"user_id\",\"article_id\",\"time_per_word\"],\n",
    "                      candidates_keep = [\"article_id\"],                      \n",
    "                      rating_target = 'time_per_word',\n",
    "                      num_epochs = 5,\n",
    "                      **kwargs):\n",
    "    start_time = time.time()\n",
    "    # convert vocabulary to str\n",
    "    candidate_df = candidate_df.reset_index()\n",
    "\n",
    "    ratings_tf =  tf.data.Dataset.from_tensor_slices(dict(rating_df[ratings_keep]))\n",
    "    candidate_tf = tf.data.Dataset.from_tensor_slices(dict(candidate_df[candidates_keep]))\n",
    "\n",
    "    candidate_ids = candidate_df[\"article_id\"].astype(str)\n",
    "    user_ids = rating_df[\"user_id\"].astype(str)\n",
    "    # candidate_ids = candidate_tf.map(lambda x: x[\"article_id\"])\n",
    "    # user_ids = ratings_tf.map(lambda x: x[\"user_id\"])\n",
    "\n",
    "    \n",
    "    unique_candidate_id = candidate_ids.unique()\n",
    "    unique_user_ids = user_ids.unique()\n",
    "    # unique_user_ids = np.unique(np.concatenate(list(user_ids)))\n",
    "\n",
    "\n",
    "    # Build vocabularies\n",
    "    user_ids_vocabulary = layers.StringLookup(\n",
    "        vocabulary=unique_user_ids, mask_token=None\n",
    "    )\n",
    "    article_ids_vocabulary = layers.StringLookup(\n",
    "        vocabulary=unique_candidate_id, mask_token=None\n",
    "    )\n",
    "\n",
    "    #SPLIT test/train\n",
    "    tf.random.set_seed(42)\n",
    "    shuffled = ratings_tf.shuffle(len(rating_df), seed=42, reshuffle_each_iteration=False)\n",
    "    train = shuffled.take(int(len(rating_df) * 0.8))\n",
    "    test = shuffled.skip(int(len(rating_df) * 0.8))\n",
    "\n",
    "\n",
    "    # model = ArticleRetrievalModel(user_ids_vocabulary=user_ids_vocabulary,\n",
    "    #                               article_ids_vocabulary=article_ids_vocabulary)\n",
    "    #LOCAL MODEL\n",
    "    embedding_dimension = 32\n",
    "    user_model = Sequential([\n",
    "                 layers.StringLookup(\n",
    "                    vocabulary=unique_user_ids, mask_token=None),\n",
    "                # We add an additional embedding to account for unknown tokens.\n",
    "                 layers.Embedding(len(unique_user_ids) + 1, embedding_dimension)\n",
    "                ])\n",
    "    \n",
    "    candidate_model = Sequential([\n",
    "                  layers.StringLookup(\n",
    "                                    vocabulary=unique_candidate_id, mask_token=None),\n",
    "                  layers.Embedding(len(unique_candidate_id) + 1, embedding_dimension)\n",
    "    ])\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    # article_candidates = (\n",
    "    #     tf_df.batch(128)\n",
    "    #     #.map(candidate_model)\n",
    "    #     .map(model.article_id_model)\n",
    "    #     # .map(lambda x: (x[\"article_id\"], model.article_embedding(x)))\n",
    "    #     .cache()\n",
    "    # )\n",
    "    article_candidates = candidate_tf.batch(128).map(\n",
    "                                                    lambda x: candidate_model(x[\"article_id\"])\n",
    "                                                ).cache()\n",
    "    \n",
    "    candidate_embeddings = candidate_model(\n",
    "        tf.convert_to_tensor(candidate_df[\"article_id\"].astype(str))\n",
    "    )\n",
    "    candidate_dataset = tf.data.Dataset.from_tensor_slices({\n",
    "    \"article_id\": candidate_df[\"article_id\"].astype(str),\n",
    "    \"embedding\": candidate_embeddings  # Your pre-computed embeddings\n",
    "    })\n",
    "\n",
    "    # Assign metrics\n",
    "    # metrics=tfrs.metrics.FactorizedTopK(candidates=article_candidates)\n",
    "\n",
    "    # Explicitly specify k values and use the brute-force method\n",
    "    # metrics = tfrs.metrics.FactorizedTopK(\n",
    "    #     candidates=article_candidates,\n",
    "    #     ks=[1, 5, 10]  # Add this line\n",
    "    # )\n",
    "    # loss=CategoricalCrossentropy()\n",
    "\n",
    "    task = tfrs.tasks.Retrieval(\n",
    "        # metrics=metrics,\n",
    "        # loss = loss\n",
    "        \n",
    "    )\n",
    "\n",
    "    model = ArticldeModel(user_model,candidate_model,task)\n",
    "\n",
    "    # optimizer=Adagrad(learning_rate=0.1)\n",
    "    # optimizer=tf.compat.v1.train.AdagradOptimizer(learning_rate=0.1)\n",
    "    optimizer='adam'\n",
    "    model.compile(optimizer=optimizer,\n",
    "                #   loss=loss, \n",
    "                #   metrics=metrics\n",
    "                  )\n",
    "    \n",
    "    monitor=kwargs.get('monitor','val_loss')\n",
    "\n",
    "    callbacks = []\n",
    "    if kwargs.get('use_tensorboard',False):\n",
    "        tb = TensorBoard(log_dir='logs', write_graph=True)\n",
    "        callbacks.append(tb)\n",
    "    if kwargs.get('use_checkpoint',False):\n",
    "        mc = ModelCheckpoint(\n",
    "                            mode='max', \n",
    "                            filepath='models-dr/pdilated.weights.h5', \n",
    "                            monitor=monitor, \n",
    "                            save_best_only='True', \n",
    "                            save_weights_only='True', \n",
    "                            verbose=1)\n",
    "        callbacks.append(mc)\n",
    "    es = EarlyStopping(\n",
    "                        monitor=monitor,\n",
    "                        mode='min',\n",
    "                    #    mode='max', \n",
    "                    #    monitor='acc', \n",
    "                        patience= kwargs.get('patience',2), \n",
    "                        verbose=1)\n",
    "    callbacks.append(es)\n",
    "\n",
    "    cached_train = train.batch(8192).cache()\n",
    "    cached_test = test.batch(4096).cache()\n",
    "\n",
    "    # model.fit(cached_train, epochs=5)\n",
    "    with mlflow.start_run() as run:\n",
    "\n",
    "        # mlflow.tensorflow.autolog()\n",
    "        history =  model.fit( #fit_generator deprecated\n",
    "                    cached_train,\n",
    "                    # steps_per_epoch=train_step_per_epoch,\n",
    "                    epochs=num_epochs,\n",
    "                    # verbose=1,\n",
    "                    validation_data=cached_test,\n",
    "                    # validation_steps=val_step_per_epoch,\n",
    "                    # use_multiprocessing=True,\n",
    "                    # workers=16,\n",
    "                    # callbacks=callbacks,\n",
    "                    # max_queue_size=32,\n",
    "                    )\n",
    "\n",
    "        process_time = time.time() - start_time\n",
    "\n",
    "\n",
    "        # Start an MLflow run\n",
    "\n",
    "        signature = None\n",
    "        # Infer the model signature\n",
    "        # if sign_model:\n",
    "        #     signature = infer_signature(X_train, model.predict(X_train), model_params )\n",
    "\n",
    "\n",
    "        model_info  = mlflow.keras.log_model(\n",
    "                                    model=model,        \n",
    "                                    name=model.name,\n",
    "                                    signature=signature,\n",
    "                                    input_example=None,\n",
    "                                    registered_model_name=f\"{model.name}\",\n",
    "                                    )\n",
    "\n",
    "        \n",
    "        # hash_id = None\n",
    "        # try:\n",
    "        #     import hashlib\n",
    "        #     hash_id = hashlib.sha256(df.to_string().encode()).hexdigest()\n",
    "        # except:\n",
    "        #     pass\n",
    "\n",
    "        # Log other information about the model\n",
    "        mlflow.log_params({ \"Process_Time\": process_time,\n",
    "                        #    'ModelParams' : model_params,\n",
    "                            'optimizer':optimizer,\n",
    "                            # 'loss':loss,\n",
    "                            'monitor':monitor,\n",
    "                            # 'GenParams' : gen_params,\n",
    "                            'Metrics' : metrics,\n",
    "                            # 'TrainStepPerEpoch' : train_step_per_epoch, #sample/batch_size\n",
    "                            # 'ValStepPerEpoch' : val_step_per_epoch,\n",
    "                            'Epochs' : num_epochs,\n",
    "                            # 'DataHash': hash_id,\n",
    "                            # 'dataset_path':df_path,\n",
    "                            # 'dataset_length':len(df.index),\n",
    "                            \n",
    "                            })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf32cac2",
   "metadata": {},
   "source": [
    "#### tfrs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0a5ad9",
   "metadata": {},
   "source": [
    "https://github.com/tensorflow/recommenders/issues/712 Bug sur la librairie tfrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de0d6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mlflow_experiment(rating_df = rating_df,\n",
    "                candidate_df = candidate_df,\n",
    "                ratings_keep= [\"user_id\",\"article_id\",\"time_per_word\"],\n",
    "                candidates_keep = [\"article_id\"],                      \n",
    "                rating_target = 'time_per_word',\n",
    "                num_epochs = 5,\n",
    "                )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d78dd4",
   "metadata": {},
   "source": [
    "##### Get recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13892de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create article dataset with embeddings\n",
    "article_dataset = tf_df.batch(128).map(lambda x: (\n",
    "    x[\"article_id\"], model.article_embedding(x)\n",
    "))\n",
    "\n",
    "index = tfrs.layers.factorized_top_k.BruteForce(model.user_model)\n",
    "index.index_from_dataset(article_dataset)\n",
    "\n",
    "user_id = \"1\"\n",
    "scores, ids = index(tf.constant([user_id]))\n",
    "print(\"Top recommendations for user\", user_id, \":\", ids[0, :5].numpy())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projet3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
